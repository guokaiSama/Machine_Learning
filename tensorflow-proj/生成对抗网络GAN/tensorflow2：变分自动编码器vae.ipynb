{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 变分自动编码器\n",
    "变分编码器是自动编码器的升级版本，其结构跟自动编码器是类似的，也由编码器和解码器构成。\n",
    "\n",
    "回忆一下，自动编码器有个问题，就是并不能任意生成图片，因为我们没有办法自己去构造隐藏向量，需要通过一张图片输入编码我们才知道得到的隐含向量是什么，这时我们就可以通过变分自动编码器来解决这个问题。\n",
    "\n",
    "其实原理特别简单，只需要在编码过程给它增加一些限制，迫使其生成的隐含向量能够粗略的遵循一个标准正态分布，这就是其与一般的自动编码器最大的不同。\n",
    "\n",
    "这样我们生成一张新图片就很简单了，我们只需要给它一个标准正态分布的随机隐含向量，这样通过解码器就能够生成我们想要的图片，而不需要给它一张原始图片先编码。\n",
    "\n",
    "一般来讲，我们通过 encoder 得到的隐含向量并不是一个标准的正态分布，为了衡量两种分布的相似程度，我们使用 KL divergence，利用其来表示隐含向量与标准正态分布之间差异的 loss，另外一个 loss 仍然使用生成图片与原图片的均方误差来表示。\n",
    "\n",
    "KL divergence 的公式如下\n",
    "\n",
    "$$\n",
    "D{KL} (P || Q) =  \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重参数\n",
    "为了避免计算 KL divergence 中的积分，我们使用重参数的技巧，不是每次产生一个隐含向量，而是生成两个向量，一个表示均值，一个表示标准差，这里我们默认编码之后的隐含向量服从一个正态分布的之后，就可以用一个标准正态分布先乘上标准差再加上均值来合成这个正态分布，最后 loss 就是希望这个生成的正态分布能够符合一个标准正态分布，也就是希望均值为 0，方差为 1\n",
    "\n",
    "所以标准的变分自动编码器如下\n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tKfTcgy1fn15cq6n7pj30k007t0sv.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以最后我们可以将我们的 loss 定义为下面的函数，由均方误差和 KL divergence 求和得到一个总的 loss\n",
    "\n",
    "```\n",
    "def loss_fun(recon_x, x, mean, std, eps=1e-8):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: original images\n",
    "    mean: latent mean\n",
    "    var: latent var\n",
    "    \"\"\"\n",
    "    mse = tf.reduce_sum(tf.square(x - recon_x))\n",
    "    # 0.5 * sum(mu^2 + std^2 - 2log(std) - 1)\n",
    "    kld_element = tf.square(mean) + tf.square(std) - 2.0 * tf.log(std + eps) - 1\n",
    "    kld = 0.5 * tf.reduce_sum(kld_element)\n",
    "    \n",
    "    return mse + kld\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们用 mnist 数据集来简单说明一下变分自动编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "\n",
    "tf.set_random_seed(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-5a7aea95e0e0>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data')\n",
    "train_set = mnist.train\n",
    "test_set = mnist.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ph = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "inputs = tf.divide(input_ph - 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae(inputs, scope='vae', reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        with slim.arg_scope([slim.fully_connected], activation_fn = tf.nn.relu):\n",
    "            # 编码\n",
    "            with tf.variable_scope('encoder'):\n",
    "                encode = slim.fully_connected(inputs, 400, scope='fc1')\n",
    "                mean = slim.fully_connected(encode, 20, activation_fn=None, scope='fc2_mean')\n",
    "                logvar = slim.fully_connected(encode, 20, activation_fn=None, scope='fc2_var')\n",
    "                \n",
    "            # 重新参数化成正态分布\n",
    "            with tf.variable_scope('reparametrize'):\n",
    "                std = tf.sqrt(tf.exp(logvar))\n",
    "                eps = tf.random_normal([20,], name='epsilon')\n",
    "                rep = eps * std + mean\n",
    "                \n",
    "            # 解码\n",
    "            with tf.variable_scope('decoder'):\n",
    "                decode = slim.fully_connected(rep, 400, scope='fc3')\n",
    "                decode = slim.fully_connected(decode, 784, activation_fn=tf.tanh, scope='fc4')\n",
    "                \n",
    "            return decode, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, mean, std = vae(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(recon_x, x, mean, std, eps=1e-8):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: original images\n",
    "    mean: latent mean\n",
    "    var: latent var\n",
    "    \"\"\"\n",
    "    # mse\n",
    "    mse = tf.reduce_sum(tf.square(x - recon_x))\n",
    "    \n",
    "    # kl divergence\n",
    "    kld_element = tf.square(mean) + tf.square(std) - 2.0 * tf.log(std + eps) - 1\n",
    "    kld = 0.5 * tf.reduce_sum(kld_element)\n",
    "    \n",
    "    return mse + kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fun(outputs, inputs, mean, std)\n",
    "\n",
    "opt = tf.train.AdamOptimizer(1e-3)\n",
    "train_op = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = tf.reshape(input_ph, (-1, 28, 28, 1))\n",
    "pre = tf.reshape(outputs, (-1, 28, 28, 1)) * 0.5 + 0.5\n",
    "images = tf.concat([gt, pre], axis=2)[:3]\n",
    "images_sum = tf.summary.image('images', images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_writer = tf.summary.FileWriter('vae/train', sess.graph)\n",
    "val_writer = tf.summary.FileWriter('vae/val', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, train_loss: 6397.56104, val_loss: 9559.11719\n",
      "Epoch: 40, train_loss: 6330.73193, val_loss: 9738.88281\n",
      "Epoch: 60, train_loss: 6132.69922, val_loss: 9306.89160\n",
      "Epoch: 80, train_loss: 6230.46191, val_loss: 8693.73242\n",
      "Epoch: 100, train_loss: 6479.74707, val_loss: 9665.60938\n"
     ]
    }
   ],
   "source": [
    "for e in range(100):\n",
    "    num_examples = 0\n",
    "    while num_examples < train_set.num_examples:\n",
    "        if num_examples + 128 < train_set.num_examples:\n",
    "            batch = 128\n",
    "        else:\n",
    "            batch = train_set.num_examples - num_examples\n",
    "        num_examples += batch\n",
    "        train_imgs, _ = train_set.next_batch(batch)\n",
    "        sess.run(train_op, feed_dict={input_ph: train_imgs})\n",
    "    if (e + 1) % 20 == 0:\n",
    "        train_imgs_sum, train_loss = sess.run([images_sum, loss], feed_dict={input_ph: train_imgs})\n",
    "        train_writer.add_summary(train_imgs_sum)\n",
    "        val_imgs, _ = test_set.next_batch(128)\n",
    "        val_imgs_sum, val_loss = sess.run([images_sum, loss], feed_dict={input_ph: val_imgs})\n",
    "        val_writer.add_summary(val_imgs_sum)\n",
    "        print('Epoch: {}, train_loss: {:.5f}, val_loss: {:.5f}'.format(e + 1, train_loss, val_loss))\n",
    "train_writer.close()\n",
    "val_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看看使用变分自动编码器得到的结果，可以发现效果比一般的编码器要好很多\n",
    "\n",
    "<img src=\"https://image.ibb.co/dY0GtH/variant_autoencoder.png\">\n",
    "\n",
    "我们可以输出其中的均值看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05094688  0.94372416 -0.16089399  0.08271772  1.5460544  -0.3548343\n",
      "   0.77291554  0.66853166 -0.49955258  0.9151107   0.24803586  1.1626108\n",
      "   1.8736731   1.1006085  -2.1362917  -0.57867366  1.5790343   0.26604328\n",
      "  -1.5685244  -1.1669159 ]]\n"
     ]
    }
   ],
   "source": [
    "imgs, _ = train_set.next_batch(1)\n",
    "mean_value = sess.run(mean, feed_dict={input_ph: imgs})\n",
    "print(mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变分自动编码器虽然比一般的自动编码器效果要好，而且也限制了其输出的编码 (code) 的概率分布，但是它仍然是通过直接计算生成图片和原始图片的均方误差来生成 loss，这个方式并不好，在下一章生成对抗网络中，我们会讲一讲这种方式计算 loss 的局限性，然后会介绍一种新的训练办法，就是通过生成对抗的训练方式来训练网络而不是直接比较两张图片的每个像素点的均方误差"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
