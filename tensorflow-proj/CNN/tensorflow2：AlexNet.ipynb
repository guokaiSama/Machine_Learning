{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet\n",
    "2010年开始举办的[ILSVRC](http://image-net.org/challenges/LSVRC/)比赛基于一个百万量级的图片数据集, 提出一个图像1000分类的挑战. 前两年在比赛中脱颖而出的都是经过人工挑选特征, 再通过`SVM`或者`随机森林`这样在过去十几年中非常成熟的机器学习方法进行分类的算法. \n",
    "\n",
    "在2012年, 由 [Alex Krizhevsky](https://www.cs.toronto.edu/~kriz/), [Ilya Sutskever](http://www.cs.toronto.edu/~ilya/), [Geoffrey Hinton](http://www.cs.toronto.edu/~hinton/)提出了一种使用卷积神经网络的方法, 以 [0.85](http://image-net.org/challenges/LSVRC/2012/results.html#abstract) 的`top-5`正确率一举获得当年分类比赛的冠军, 超越使用传统方法的第二名10个百分点, 震惊了当时的学术界, 从此开启了人工智能领域的新篇章."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CIFAR10`\n",
    "\n",
    "这是一个包含60000张$32\\times32$图片的数据库, 包含50000张训练集和10000张测试集, 这里我们提供一个脚本帮助我们读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from utils import cifar10_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们定义一个批次有64个样本\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先下载数据\n",
    "#链接：https://pan.baidu.com/s/1fbThmv8LDhLRemA62m3nlQ \n",
    "#提取码：j1d8 \n",
    "\n",
    "# 获取训练集\n",
    "# 在使用随机梯度下降法的时候, 训练集要求打乱样本\n",
    "data_dir = 'cifar10_data'\n",
    "\n",
    "train_imgs, train_labels = cifar10_input.inputs(eval_data=False, \n",
    "                                                data_dir='./cifar10_data/cifar-10-batches-bin/', \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=True)\n",
    "\n",
    "# 获取测试集\n",
    "# 测试集不需要打乱样本\n",
    "val_imgs, val_labels = cifar10_input.inputs(eval_data=True, \n",
    "                                            data_dir='./cifar10_data/cifar-10-batches-bin/', \n",
    "                                            batch_size=batch_size, \n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN # 训练样本的个数\n",
    "val_examples = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL       # 测试样本的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 像之前一样, 我们构造几个生成变量的函数\n",
    "def variable_weight(shape, stddev=5e-2):\n",
    "    init = tf.truncated_normal_initializer(stddev=stddev)\n",
    "    return tf.get_variable(shape=shape, initializer=init, name='weight')\n",
    "\n",
    "def variable_bias(shape):\n",
    "    init = tf.constant_initializer(0.1)\n",
    "    return tf.get_variable(shape=shape, initializer=init, name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(x, ksize, out_depth, strides, padding='SAME', act=tf.nn.relu, scope='conv_layer', reuse=None):\n",
    "    \"\"\"构造一个卷积层\n",
    "    Args:\n",
    "        x: 输入\n",
    "        ksize: 卷积核的大小, 一个长度为2的`list`, 例如[3, 3]\n",
    "        output_depth: 卷积核的个数\n",
    "        strides: 卷积核移动的步长, 一个长度为2的`list`, 例如[2, 2]\n",
    "        padding: 卷积核的补0策略\n",
    "        act: 完成卷积后的激活函数, 默认是`tf.nn.relu`\n",
    "        scope: 这一层的名称(可选)\n",
    "        reuse: 是否复用\n",
    "    \n",
    "    Return:\n",
    "        out: 卷积层的结果\n",
    "    \"\"\"\n",
    "    # 这里默认数据是NHWC输入的\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 先构造卷积核\n",
    "        shape = ksize + [in_depth, out_depth]\n",
    "        with tf.variable_scope('kernel'):\n",
    "            kernel = variable_weight(shape)\n",
    "            \n",
    "        strides = [1, strides[0], strides[1], 1]\n",
    "        # 生成卷积\n",
    "        conv = tf.nn.conv2d(x, kernel, strides, padding, name='conv')\n",
    "        \n",
    "        # 构造偏置\n",
    "        with tf.variable_scope('bias'):\n",
    "            bias = variable_bias([out_depth])\n",
    "            \n",
    "        # 和偏置相加\n",
    "        preact = tf.nn.bias_add(conv, bias)\n",
    "        \n",
    "        # 添加激活层\n",
    "        out = act(preact)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(x, ksize, strides, padding='SAME', name='pool_layer'):\n",
    "    \"\"\"构造一个最大值池化层\n",
    "    Args:\n",
    "        x: 输入\n",
    "        ksize: pooling核的大小, 一个长度为2的`list`, 例如[3, 3]\n",
    "        strides: pooling核移动的步长, 一个长度为2的`list`, 例如[2, 2]\n",
    "        padding: pooling的补0策略\n",
    "        name: 这一层的名称(可选)\n",
    "    \n",
    "    Return:\n",
    "        pooling层的结果\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(x, [1, ksize[0], ksize[1], 1], [1, strides[0], strides[1], 1], padding, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(x, out_depth, act=tf.nn.relu, scope='fully_connect', reuse=None):\n",
    "    \"\"\"构造一个全连接层\n",
    "    Args:\n",
    "        x: 输入\n",
    "        out_depth: 输出向量的维数\n",
    "        act: 激活函数, 默认是`tf.nn.relu`\n",
    "        scope: 名称域, 默认是`fully_connect`\n",
    "        reuse: 是否需要重用\n",
    "    \"\"\"\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    \n",
    "    # 构造全连接层的参数\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 构造权重\n",
    "        with tf.variable_scope('weight'):\n",
    "            weight = variable_weight([in_depth, out_depth])\n",
    "            \n",
    "        # 构造偏置项\n",
    "        with tf.variable_scope('bias'):\n",
    "            bias = variable_bias([out_depth])\n",
    "        \n",
    "        # 一个线性函数\n",
    "        fc = tf.nn.bias_add(tf.matmul(x, weight), bias, name='fc')\n",
    "        \n",
    "        # 激活函数作用\n",
    "        out = act(fc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(inputs, reuse=None):\n",
    "    \"\"\"构建 Alexnet 的前向传播\n",
    "    Args:\n",
    "        inpus: 输入\n",
    "        reuse: 是否需要重用\n",
    "        \n",
    "    Return:\n",
    "        net: alexnet的结果\n",
    "    \"\"\"\n",
    "    # 首先我们声明一个变量域`AlexNet`\n",
    "    with tf.variable_scope('AlexNet', reuse=reuse):\n",
    "        # 第一层是 5x5 的卷积, 卷积核的个数是64, 步长是 1x1, padding是`VALID`\n",
    "        net = conv(inputs, [5, 5], 64, [1, 1], padding='VALID', scope='conv1')\n",
    "        \n",
    "        # 第二层是 3x3 的池化, 步长是 2x2, padding是`VALID`\n",
    "        net = max_pool(net, [3, 3], [2, 2], padding='VALID', name='pool1')\n",
    "        \n",
    "        # 第三层是 5x5 的卷积, 卷积核的个数是64, 步长是 1x1, padding是`VALID`\n",
    "        net = conv(net, [5, 5], 64, [1, 1], scope='conv2')\n",
    "        \n",
    "        # 第四层是 3x3 的池化, 步长是 2x2, padding是`VALID`\n",
    "        net = max_pool(net, [3, 3], [2, 2], padding='VALID', name='pool2')\n",
    "        \n",
    "        # 将矩阵拉长成向量\n",
    "        net = tf.reshape(net, [-1, 6*6*64])\n",
    "        \n",
    "        # 第五层是全连接层, 输出个数为384\n",
    "        net = fc(net, 384, scope='fc3')\n",
    "        \n",
    "        # 第六层是全连接层, 输出个数为192\n",
    "        net = fc(net, 192, scope='fc4')\n",
    "        \n",
    "        # 第七层是全连接层, 输出个数为10, 注意这里不要使用激活函数\n",
    "        net = fc(net, 10, scope='fc5', act=tf.identity)\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 alexnet 构建训练和测试的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out = alexnet(train_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意当再次调用 alexnet 函数时, 如果要使用之前调用时产生的变量值, **必须**要重用变量域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_out = alexnet(val_imgs, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义损失函数\n",
    "这里真实的 labels 不是一个 one_hot 型的向量, 而是一个数值, 因此我们使用 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    train_loss = tf.losses.sparse_softmax_cross_entropy(labels=train_labels, logits=train_out, scope='train')\n",
    "    val_loss = tf.losses.sparse_softmax_cross_entropy(labels=val_labels, logits=val_out, scope='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义正确率`op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('train'):\n",
    "        train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_out, axis=-1, output_type=tf.int32), train_labels), tf.float32))\n",
    "    with tf.name_scope('train'):\n",
    "        val_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(val_out, axis=-1, output_type=tf.int32), val_labels), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构造训练`op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "opt = tf.train.MomentumOptimizer(lr, momentum=0.9)\n",
    "train_op = opt.minimize(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.learning import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]: step 0 loss = nan acc = 0.0000 (0.0083 / batch)\n",
      "[val]: step 0 loss = nan acc = 0.0156\n",
      "[train]: step 1000 loss = nan acc = 0.0000 (0.0212 / batch)\n",
      "[train]: step 2000 loss = nan acc = 0.0000 (0.0222 / batch)\n",
      "[train]: step 3000 loss = nan acc = 0.0000 (0.0223 / batch)\n",
      "[train]: step 4000 loss = nan acc = 0.0000 (0.0224 / batch)\n",
      "[val]: step 4000 loss = nan acc = 0.0000\n",
      "[train]: step 5000 loss = nan acc = 0.0000 (0.0222 / batch)\n",
      "[train]: step 6000 loss = nan acc = 0.0000 (0.0224 / batch)\n",
      "[train]: step 7000 loss = nan acc = 0.0000 (0.0223 / batch)\n",
      "[train]: step 8000 loss = nan acc = 0.0000 (0.0224 / batch)\n",
      "[val]: step 8000 loss = nan acc = 0.0000\n",
      "[train]: step 9000 loss = nan acc = 0.0000 (0.0215 / batch)\n",
      "[train]: step 10000 loss = nan acc = 0.0000 (0.0218 / batch)\n",
      "[train]: step 11000 loss = nan acc = 0.0000 (0.0225 / batch)\n",
      "[train]: step 12000 loss = nan acc = 0.0000 (0.0228 / batch)\n",
      "[val]: step 12000 loss = nan acc = 0.0000\n",
      "[train]: step 13000 loss = nan acc = 0.0312 (0.0227 / batch)\n",
      "[train]: step 14000 loss = nan acc = 0.0000 (0.0227 / batch)\n",
      "[train]: step 15000 loss = nan acc = 0.0156 (0.0227 / batch)\n",
      "[train]: step 16000 loss = nan acc = 0.0000 (0.0225 / batch)\n",
      "[val]: step 16000 loss = nan acc = 0.0000\n",
      "[train]: step 17000 loss = nan acc = 0.0000 (0.0224 / batch)\n",
      "[train]: step 18000 loss = nan acc = 0.0000 (0.0211 / batch)\n",
      "[train]: step 19000 loss = nan acc = 0.0000 (0.0223 / batch)\n",
      "[train]: step 20000 loss = nan acc = 0.0000 (0.0227 / batch)\n",
      "[val]: step 20000 loss = nan acc = 0.0000\n",
      "-------------------------Over all Result-------------------------\n",
      "[TRAIN]: loss = nan acc = 0.0031\n",
      "[VAL]: loss = nan acc = 0.0030\n"
     ]
    }
   ],
   "source": [
    "train(train_op, train_loss, train_acc, val_loss, val_acc, 20000, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到, 20000步的训练后, `AlexNet`在训练集和测试集分别达到了`0.97`和`0.72`的准确率"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
