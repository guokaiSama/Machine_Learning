{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet\n",
    "因为 ResNet 提出了跨层链接的思想，这直接影响了随后出现的卷积网络架构，其中最有名的就是 cvpr 2017 的 best paper，DenseNet。\n",
    "\n",
    "DenseNet 和 ResNet 不同在于 ResNet 是跨层求和，而 DenseNet 是跨层将特征在通道维度进行拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为是在通道维度进行特征的拼接，所以底层的输出会保留进入所有后面的层，这能够更好的保证梯度的传播，同时能够使用低维的特征和高维的特征进行联合训练，能够得到更好的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet 主要由 dense block 构成，下面我们来实现一个 densen block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "from utils import cifar10_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_imgs, train_labels, val_imgs, val_labels = cifar10_input.load_data(image_size=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建基本卷积单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_relu_conv(x, out_depth, scope='dense_basic_conv', reuse=None):\n",
    "    # 基本卷积单元是: bn->relu-conv\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        net = slim.batch_norm(x, activation_fn=None, scope='bn')\n",
    "        net = tf.nn.relu(net, name='activation')\n",
    "        net = slim.conv2d(net, out_depth, 3, activation_fn=None, normalizer_fn=None, biases_initializer=None, scope='conv')\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建`densenet`的基本单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(x, growth_rate, num_layers, scope='dense_block', reuse=None):\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        net = x\n",
    "        for i in range(num_layers):\n",
    "            out = bn_relu_conv(net, growth_rate, scope='block%d' % i)\n",
    "            # 将前面所有的输出连接到一起作为下一个基本卷积单元的输入\n",
    "            net = tf.concat([net, out], axis=-1)\n",
    "            \n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建`transition`层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(x, out_depth, scope='transition', reuse=None):\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        net = slim.batch_norm(x, activation_fn=None, scope='bn')\n",
    "        net = tf.nn.relu(net, name='activation')\n",
    "        net = slim.conv2d(net, out_depth, 1, activation_fn=None, normalizer_fn=None, biases_initializer=None, scope='conv')\n",
    "        net = slim.avg_pool2d(net, 2, 2, scope='avg_pool')\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建`densenet`整体结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet(x, num_classes, growth_rate=32, block_layers=[6, 12, 24, 16], is_training=None, scope='densenet', reuse=None, verbose=False):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        with slim.arg_scope([slim.batch_norm], is_training=is_training):\n",
    "            \n",
    "            if verbose:\n",
    "                print('input: {}'.format(x.shape))\n",
    "            \n",
    "            with tf.variable_scope('block0'):\n",
    "                net = slim.conv2d(x, 64, [7, 7], stride=2, normalizer_fn=None, activation_fn=None, scope='conv_7x7')\n",
    "                net = slim.batch_norm(net, activation_fn=None, scope='bn')\n",
    "                net = tf.nn.relu(net, name='activation')\n",
    "                net = slim.max_pool2d(net, [3, 3], stride=2, scope='max_pool')\n",
    "\n",
    "                if verbose:\n",
    "                    print('block0: {}'.format(net.shape))\n",
    "\n",
    "            # 循环创建`dense_block`和`transition`\n",
    "            for i, num_layers in enumerate(block_layers):\n",
    "                with tf.variable_scope('block%d' % (i + 1)):\n",
    "                    net = dense_block(net, growth_rate, num_layers)\n",
    "                    if i != len(block_layers) - 1:\n",
    "                        current_depth = net.get_shape().as_list()[-1]\n",
    "                        net = transition(net, current_depth // 2)\n",
    "\n",
    "                if verbose:\n",
    "                    print('block{}: {}'.format(i+1, net.shape))\n",
    "\n",
    "            with tf.variable_scope('block%d' % (len(block_layers) + 1)):\n",
    "                net = slim.batch_norm(net, activation_fn=None, scope='bn')\n",
    "                net = tf.nn.relu(net, name='activation')\n",
    "                net = tf.reduce_mean(net, [1, 2], name='global_pool', keep_dims=True)\n",
    "\n",
    "                if verbose:\n",
    "                    print('block{}: {}'.format(len(block_layers) + 1, net.shape))\n",
    "\n",
    "            with tf.variable_scope('classification'):\n",
    "                net = slim.flatten(net, scope='flatten')\n",
    "                net = slim.fully_connected(net, num_classes, activation_fn=None, normalizer_fn=None, scope='logit')\n",
    "\n",
    "                if verbose:\n",
    "                    print('classification: {}'.format(net.shape))\n",
    "\n",
    "                return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm) as sc:\n",
    "    conv_scope = sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: (64, 96, 96, 3)\n",
      "block0: (64, 23, 23, 64)\n",
      "block1: (64, 11, 11, 128)\n",
      "block2: (64, 5, 5, 256)\n",
      "block3: (64, 2, 2, 512)\n",
      "block4: (64, 2, 2, 1024)\n",
      "WARNING:tensorflow:From <ipython-input-8-026557c48f0c>:31: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "block5: (64, 1, 1, 1024)\n",
      "classification: (64, 10)\n"
     ]
    }
   ],
   "source": [
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "with slim.arg_scope(conv_scope):\n",
    "    train_out = densenet(train_imgs, 10, is_training=is_training, verbose=True)\n",
    "    val_out = densenet(val_imgs, 10, is_training=is_training, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    train_loss = tf.losses.sparse_softmax_cross_entropy(labels=train_labels, logits=train_out, scope='train')\n",
    "    val_loss = tf.losses.sparse_softmax_cross_entropy(labels=val_labels, logits=val_out, scope='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('train'):\n",
    "        train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_out, axis=-1, output_type=tf.int32), train_labels), tf.float32))\n",
    "    with tf.name_scope('val'):\n",
    "        val_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(val_out, axis=-1, output_type=tf.int32), val_labels), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "opt = tf.train.MomentumOptimizer(lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = opt.minimize(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.learning import train_with_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]: step 0 loss = 2.3076 acc = 0.1094 (0.0392 / batch)\n",
      "[val]: step 0 loss = 2.3089 acc = 0.1719\n",
      "[train]: step 1000 loss = 1.0021 acc = 0.6875 (0.1351 / batch)\n",
      "[train]: step 2000 loss = 0.6037 acc = 0.7656 (0.1330 / batch)\n",
      "[train]: step 3000 loss = 0.4614 acc = 0.8125 (0.1330 / batch)\n",
      "[train]: step 4000 loss = 0.3103 acc = 0.9219 (0.1330 / batch)\n",
      "[val]: step 4000 loss = 0.8321 acc = 0.7500\n",
      "[train]: step 5000 loss = 0.4124 acc = 0.8594 (0.1335 / batch)\n",
      "[train]: step 6000 loss = 0.4418 acc = 0.8906 (0.1329 / batch)\n",
      "[train]: step 7000 loss = 0.2239 acc = 0.9531 (0.1327 / batch)\n",
      "[train]: step 8000 loss = 0.0356 acc = 0.9844 (0.1331 / batch)\n",
      "[val]: step 8000 loss = 0.7691 acc = 0.8125\n",
      "[train]: step 9000 loss = 0.0643 acc = 0.9688 (0.1331 / batch)\n",
      "[train]: step 10000 loss = 0.3805 acc = 0.8750 (0.1331 / batch)\n",
      "[train]: step 11000 loss = 0.1420 acc = 0.9688 (0.1330 / batch)\n",
      "[train]: step 12000 loss = 0.1092 acc = 0.9688 (0.1331 / batch)\n",
      "[val]: step 12000 loss = 1.2618 acc = 0.6875\n",
      "[train]: step 13000 loss = 0.0069 acc = 1.0000 (0.1332 / batch)\n",
      "[train]: step 14000 loss = 0.2515 acc = 0.9375 (0.1329 / batch)\n",
      "[train]: step 15000 loss = 0.0503 acc = 0.9688 (0.1329 / batch)\n",
      "[train]: step 16000 loss = 0.0017 acc = 1.0000 (0.1331 / batch)\n",
      "[val]: step 16000 loss = 0.7645 acc = 0.8438\n",
      "[train]: step 17000 loss = 0.0186 acc = 0.9844 (0.1332 / batch)\n",
      "[train]: step 18000 loss = 0.0005 acc = 1.0000 (0.1331 / batch)\n",
      "[train]: step 19000 loss = 0.0351 acc = 0.9844 (0.1332 / batch)\n",
      "[train]: step 20000 loss = 0.0278 acc = 0.9844 (0.1333 / batch)\n",
      "[val]: step 20000 loss = 0.6719 acc = 0.8125\n",
      "-------------------------Over all Result-------------------------\n",
      "[TRAIN]: loss = 0.0315 acc = 0.9891\n",
      "[VAL]: loss = 1.0283 acc = 0.8160\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_with_bn(sess, train_op, train_loss, train_acc, val_loss, val_acc, 20000, is_training)\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
