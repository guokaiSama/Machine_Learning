{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面的部分我们基本上已经把模型训练的流程讲完了，现在我们都是使用`cifar10`里的数据读取函数进行数据读入，接下来我们讲如何导入自己的数据集，这里会用到一个`tensorflow`内置的，非常有用的模块\n",
    "- [`Queue`](#Queue)\n",
    "- [`.tfrecord`](#.tfrecord)\n",
    "- [`tf.data`](#tf.data)\n",
    "\n",
    "下面我们来讲一讲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://tensorflow.google.cn/images/AnimatedFileQueues.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensorflow`提供一种队列方式进行数据的读取, 我们通过读取图片的例子来看看整体用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将图片名和标签信息读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example_data/imgs.txt', 'r') as fid:\n",
    "    lines = fid.readlines()\n",
    "    \n",
    "img_names = ['example_data/%s' % line.strip().split()[0] for line in lines]\n",
    "img_labels = [line.strip().split()[1] for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **tf.train.slice_input_producer**\n",
    "\n",
    "将输入按照第0维进行切割(可以有多个输入), 生成一个队列\n",
    "\n",
    "(注意`num_epochs`参数, 表示使用多少次全样本集, 在这里设置为1, 不设置的话默认使用无限次)\n",
    "(注意`shuffle`参数, 表示是否在一个全样本集内部打乱顺序, 默认设置为`True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_queue = tf.train.slice_input_producer([img_names, img_labels], shuffle=False, num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **解析队列**, 生成具体样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(data_queue):\n",
    "    filename = data_queue[0]\n",
    "    label = data_queue[1]\n",
    "    img_file = tf.read_file(filename)\n",
    "    img_decoded = tf.image.decode_image(img_file)\n",
    "    # 这里最好设定输出图片的形状, 否则后面无法进行`batch`操作\n",
    "    # 比如我们可以`resize`到固定大小\n",
    "    img_decoded.set_shape((32, 32, 3))\n",
    "    \n",
    "    return img_decoded, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **读取队列**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = read(data_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3) ()\n"
     ]
    }
   ],
   "source": [
    "print(img.get_shape(), label.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed to create session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6fa0ae8f41e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_pruned_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1643\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mInteractiveSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mInteractiveSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_session_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed to create session."
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意, 这里需要初始化局部变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **tf.train.Coordinator**\n",
    "\n",
    "生成一个管理器, 管理读取线程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = tf.train.Coordinator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **tf.train.start_queue_runners**\n",
    "\n",
    "启动线程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 运行输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        py_img, py_label = sess.run([img, label])\n",
    "        plt.figure(figsize=(1, 1))\n",
    "        plt.imshow(py_img)\n",
    "        plt.title(py_label)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        # 当报错越界时, 输出信息, 结束循环\n",
    "        print('Epoch Limited. Done')\n",
    "        break\n",
    "    finally:\n",
    "        # 停止读取线程\n",
    "        coord.request_stop()\n",
    "# 等待线程彻底终止\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**: 一个`session`只能开启一个队列, 在这里我们先关闭这个`sess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面大家看到了队列的基本操作, 下面我们再来看一些常用的数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **slice_input_producer: shuffle=True**\n",
    "\n",
    "在样本集内部打乱样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_queue = tf.train.slice_input_producer([img_names, img_labels], num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = read(data_queue)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    while True:\n",
    "        try:\n",
    "            py_img, py_label = sess.run([img, label])\n",
    "            print(py_label)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            # 当报错越界时, 输出信息, 结束循环\n",
    "            print('Epoch Limited. Done')\n",
    "            break\n",
    "        finally:\n",
    "            # 停止读取线程\n",
    "            coord.request_stop()\n",
    "    # 等待线程彻底终止\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **tf.train.batch**\n",
    "\n",
    "将`batch_size`个样本打包成一次输出\n",
    "\n",
    "- **tf.train.shuffle_batch**\n",
    "\n",
    "对样本进行打乱顺序然后打包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "min_after_dequeue = 1000\n",
    "capacity = min_after_dequeue + 3 * batch_size\n",
    "\n",
    "data_queue = data_queue = tf.train.slice_input_producer([img_names, img_labels], shuffle=False, num_epochs=10)\n",
    "img, label = read(data_queue)\n",
    "# 如果不需要打乱样本, 可以用\n",
    "#imgs, labels = tf.train.batch([img, label], batch_size)\n",
    "imgs, labels = tf.train.shuffle_batch([img, label], batch_size, capacity=capacity, min_after_dequeue=min_after_dequeue)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            py_imgs, py_labels = sess.run([imgs, labels])\n",
    "            print(py_imgs.shape, py_labels)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            # 当报错越界时, 输出信息, 结束循环\n",
    "            print('Epoch Limited. Done')\n",
    "            break\n",
    "        finally:\n",
    "            # 停止读取线程\n",
    "            coord.request_stop()\n",
    "    # 等待线程彻底终止\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于队列的基础知识就介绍到这, 接下来我们来看看`tensorflow`为了在内部高效化读取数据而定义的一种全新文件格式\n",
    "\n",
    "## .tfrecord\n",
    "\n",
    "`.tfreocrd`是`tensorflow`特有的数据存储形式. 在使用的时候, 第一步需要将我们自己的数据转换成`.tfrecord`格式的文件, 在之后我们就可以从相应的`.tfrecord`文件中解码读取. 由于`tensorflow`为`.tfrecord`定制了很多读取函数, 因此它比原生的从硬盘中读取的方式效率高一些."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成`.tfrecord`文件\n",
    "\n",
    "`.tfrecord`文件包含了`tf.train.Example`协议缓冲区, 我们首先需要定义`writer`用来往文件里写入, 然后将数据转换成特定形式, 再调用`writer`进行写入就完成了\n",
    "- - -\n",
    "数据需要转换成**`tf.train.Features()`**的形式, 这是一个字典, \n",
    "\n",
    "- `key`值是数据的名字\n",
    "\n",
    "用来处理不同类型的数据, 比如图片和标签就可以分别存诚`img`, `label`两个部分.\n",
    "\n",
    "- `value`是`tf.train.Feature()`形式的特征\n",
    "\n",
    "而我们要做的就是把每个单独的数据转换成这种特征. \n",
    "\n",
    "- - -\n",
    "\n",
    "特征有3种:\n",
    "\n",
    "- bytes_list 将字符串数据存储在这里\n",
    "- int64_list 将整型标量(也就是一个数)存储在这里\n",
    "- float_list 将浮点型标量存储在这里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_fname = './example_data.tfrecord'\n",
    "writer = tf.python_io.TFRecordWriter(tfrecord_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_name, img_label in zip(img_names, img_labels):\n",
    "    # 读取图片\n",
    "    img_raw = cv2.imread(img_name)\n",
    "    # 将图片数组转换成字符串形式, 后面可以解码\n",
    "    img_raw = img_raw.tostring()\n",
    "    \n",
    "    # 定义一个样本\n",
    "    example = tf.train.Example(features=tf.train.Features(\n",
    "        # 定义特征字典\n",
    "        feature={\n",
    "            # 将`img_label`作为'img_label'的值存入样本中, 这里它是一个字符串, 所以我们用`bytes_list`\n",
    "            'img_label': tf.train.Feature(bytes_list = tf.train.BytesList(value=[img_label.encode()])), \n",
    "            # 将`img_raw`作为'img_raw'的值存入样本中, 图片已经转换成了字符串, 同理`bytes_list`\n",
    "            'img_raw': tf.train.Feature(bytes_list = tf.train.BytesList(value=[img_raw]))\n",
    "        }))\n",
    "    # 将样本序列化成字符串后写入`.tfrecord`文件中\n",
    "    writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关闭读写器\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时候我们就发现在当前目录下多了一个`example_data.tfrecord`的文件.\n",
    "\n",
    "从上面的过程我们就可以发现, `.tfrecord`可以将一个样本的所有信息整合在一起, 非常方便"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取`.tfrecord`文件\n",
    "\n",
    "现在我们再来看看如何读取`.tfrecord`文件到内存中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 生成一个文件名队列, 这个队列只有一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(['example_data.tfrecord'], num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义一个读取器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = tf.TFRecordReader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 返回文件名和文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, serialized_example = reader.read(filename_queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 按照指定特征解析`example`里面的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf.parse_single_example(serialized_example, \n",
    "                                   features={\n",
    "                                       'img_label': tf.FixedLenFeature([], tf.string), \n",
    "                                       'img_raw': tf.FixedLenFeature([], tf.string)\n",
    "                                   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.decode_raw(features['img_raw'], tf.uint8)\n",
    "img = tf.reshape(img, (32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = features['img_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    while True:\n",
    "        try:\n",
    "            py_img, py_label = sess.run([img, label])\n",
    "            print(py_img.shape, py_label)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            # 当报错越界时, 输出信息, 结束循环\n",
    "            print('Epoch Limited. Done')\n",
    "            break\n",
    "        finally:\n",
    "            # 停止读取线程\n",
    "            coord.request_stop()\n",
    "    # 等待线程彻底终止\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取完全正确, 关于`.tfrecord`还有很多内容没有详述, 大家可以参考下面几个链接继续深入学习:\n",
    "- https://tensorflow.google.cn/versions/r1.2/programmers_guide/reading_data\n",
    "- http://blog.csdn.net/u010223750/article/details/70482498\n",
    "\n",
    "接下来为大家介绍`tf-1.3`版本纳入`contrib`中, `tf-1.4`版本正式纳入核心库的`tf.data`模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data\n",
    "> `tf.data`可以帮助我们更轻松地处理超量级, 不同格式, 需要进行复杂变换的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`tf.data`由两个部分构成:\n",
    "- 构建一个数据集(`tf.data.Dataset`)\n",
    "- 从数据集中获取元素(`tf.data.Iterator`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用法\n",
    "我们先用通过一个`numpy`的一维数组构建和使用`dataset`做为例子来看看整体用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 构建一个`[0, 5)之间长度为5的数组`\n",
    "x = np.random.randint(0, 5, size=5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建数据集\n",
    "从`x`构建一个`dataset`,它第`i`个元素正是`x`的第`i`个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看数据集的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.output_types)\n",
    "print(dataset.output_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成一个在数据集上的迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取数据集中的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_elm = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 迭代读取数据集中的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(sess.run(next_elm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**`x`有5个元素, 如果我们跑5次以上迭代的话就会报越界的错误"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现, 加入了迭代器机制后读取数据变得非常简单优雅. 下面再介绍关于`dataset`的其他基本用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多种数据构成的数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新构造迭代器和获取元素的`op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_run(sess, dataset, max_step):\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_elm= iterator.get_next()\n",
    "\n",
    "    for i in range(max_step):\n",
    "        # 如果报越界错误, 打印信息并退出循环\n",
    "        try:\n",
    "            print(sess.run(next_elm))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Epoch limited, done')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`tf.data.Dataset.zip`**\n",
    "\n",
    "将两个`dataset`进行拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先定义一个`[5, 2]`的数据集, 和一个包含5个字符串的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(np.random.rand(5, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_run(sess, dataset1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(['one', 'two', 'three', 'four', 'five'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后把`dataset1`和`dataset2`通过`tf.data.Dataset.zip`函数连接在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.zip((dataset1, dataset2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.output_shapes)\n",
    "print(dataset.output_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_run(sess, dataset, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 直接在`tf.data.Dataset.from_tensor_slices`中定义两个数据集\n",
    "\n",
    "还可以用字典的形式给数据加名字用来区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    {'a': np.random.rand(5, 2), \n",
    "     'b': np.random.randint(0, 2, [5])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.output_types)\n",
    "print(dataset.output_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对数据集进行变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个元素+1\n",
    "def add_one(x):\n",
    "    return x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`dataset.map`**\n",
    "\n",
    "类似`python`下的`map`函数, `dataset`的`map`函数也有相同的功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.random.randint(0, 5, [5]))\n",
    "dataset = dataset.map(lambda x: add_one(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_run(sess, dataset, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`dataset.filter`**\n",
    "\n",
    "类似`python`下的`filter`函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 留下小于3的元素\n",
    "dataset_filtered = dataset.filter(lambda x: tf.less(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_run(sess, dataset_filtered, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`dataset.flat_map`**\n",
    "\n",
    "和`dataset.map`功能前面功能相同, 后面会把结果展开成一个向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`dataset.repeat`**\n",
    "\n",
    "上面的操作只能读取一次样本集, `repeat`函数能够帮助我们任意次读取样本集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_run(sess, dataset, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你想要读取`n`次样本集, 使用\n",
    "\n",
    "`dataset = dataset.repeat(n)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`dataset.shuffle`**\n",
    "\n",
    "打乱数据集样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_run(sess, dataset, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`dataset.batch`**\n",
    "\n",
    "一次读取`batch_size`个样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_run(sess, dataset, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面介绍的这些`tf.data`的基本方法可以满足我们大部分时候的需求了, 我们再用读取图片数据为例子作为本章的结束"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取图片文件名和标签名列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example_data/imgs.txt') as fid:\n",
    "    lines = fid.readlines()\n",
    "    \n",
    "filenames = ['example_data/%s' % line.strip().split()[0] for line in lines]\n",
    "labels = [line.strip().split()[1] for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "独立构造图片数据集以及标签数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "label_dataset = tf.data.Dataset.from_tensor_slices(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里`image_dataset`的元素是一个字符串, 我们需要将它转化成图片本身"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(name):\n",
    "    img_file = tf.read_file(name)\n",
    "    img_decoded = tf.image.decode_image(img_file, channels=3)\n",
    "    return img_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = image_dataset.map(lambda name: read_img(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以对图片进行变换, 也就是预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distort_img(img):\n",
    "    img_flip_lr = tf.image.random_flip_left_right(img)\n",
    "    img_flip_ud = tf.image.random_flip_up_down(img_flip_lr)\n",
    "    img_adj_bri = tf.image.random_brightness(img_flip_ud, 0.5)\n",
    "    img_adj_con = tf.image.random_contrast(img_adj_bri, 0.5, 1)\n",
    "    \n",
    "    return img_adj_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = image_dataset.map(lambda img: distort_img(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在将两部分数据融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.zip((image_dataset, label_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定循环样本集10次\n",
    "dataset = dataset.repeat(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打乱样本\n",
    "dataset = dataset.shuffle(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定`batch_size`\n",
    "dataset = dataset.batch(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在数据集以及处理完成, 我们来看看实际效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_visualize(sess, dataset, max_step):\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    images, labels = iterator.get_next()\n",
    "\n",
    "    for i in range(max_step):\n",
    "        try:\n",
    "            np_imgs, np_labels = sess.run([images, labels])\n",
    "            _, axes = plt.subplots(1, 5, figsize=(8, 8))\n",
    "            for n in range(5):\n",
    "                axes[n].imshow(np_imgs[n])\n",
    "                axes[n].set_title(np_labels[n])\n",
    "                axes[n].axes.get_xaxis().set_visible(False)\n",
    "                axes[n].axes.get_yaxis().set_visible(False)\n",
    "            plt.show()\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Epoch limited, done')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_visualize(sess, dataset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结语"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们学习了如何使用`tensorflow`的各种方法方便地进行数据的读取和处理, 下面我们再看看如何在自己的数据集上训练以及一些技巧"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
