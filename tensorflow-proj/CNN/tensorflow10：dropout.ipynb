{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropout\n",
    "前面我们讲了数据增强，看到了数据增强对模型结果具有非常明显的改善，对于改善过拟合有非常明显的效果，除了数据增强之外，改善过拟合的办法还有 dropout。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dropout\n",
    "dropout 的灵感来源于人脑的神经元发育，从幼年时候发展到青年时期，人脑的神经元会将一些不使用的逐渐去掉，而发展一些新的神经元，dropout 依据这个原理，在训练的时候以概率 p 保留每个神经元，也就是说在训练的时候，每次都会有神经元被随机设置为 0，如下图\n",
    "\n",
    "![](https://ws3.sinaimg.cn/large/006tNc79ly1fmrrdkid5aj30h008uq4c.jpg)\n",
    "\n",
    "左边是标准的神经网络，稠密连接，而右边就是使用了 dropout 的稀疏连接。我们可以看到每次训练的时候就会有某些神经元没有参与训练，所以在每个 batch 进行训练的时候模型都会有微小的区别，比如\n",
    "\n",
    "![](https://ws3.sinaimg.cn/large/006tNc79ly1fmrrlep23fj30g609st9f.jpg)\n",
    "\n",
    "这个时候就会出现一个新的问题，测试的时候怎么办呢？如果我们使用 dropout，那么测试得到的结果就是随机的，没有确定性，而不使用 dropout 网络的输出肯定跟使用 dropout 的结果不一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如使用 dropout 之前的输入是 x，那么使用完 dropout 之后输出的期望就是 $px + (1-p)0 = px$，也就是说 $x \\rightarrow px$。为了保证结果相同，非常简单，对输出做一下缩放，乘上 $\\frac{1}{p}$ 就可以了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensorflow`中使用`dropout`的方法非常简答，使用`tf.nn.dropuout`就行了，第一个参数是输入, 第二个参数`keep_prob`表示保留的概率。对于 dropout 在训练和测试时候的表现不同，只需要将预测时的`dropout`改成`1.0`即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般 dropout 会用在全连接层，但是由于现在卷积网络逐渐去掉全连接层，所以现在 dropout 在卷积网络中使用的非常的少，同时有更好的技术进行替代，就是下面要讲的正则化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
