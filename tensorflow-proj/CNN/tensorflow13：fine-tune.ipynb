{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通过微调进行迁移学习\n",
    "\n",
    "`tensorflow`中没有预训练好的模型, 但在`tf-slim`中包含了很多著名网络的网络结构图和预训练模型, 比如我们前面介绍过的`alexnet`,`vgg`,`inception`,`resnet`以及它们的一些变体. 在使用的时候我们先用`tf-slim`定义好的网络构建计算图, 然后再加载预训练模型的参数就行了, 非常简单.\n",
    "\n",
    "下面我们用一个例子来演示一些微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from utils.custom_input import read\n",
    "from utils.learning import train_with_bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先来看一下数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = './hymenoptera_data/train/'\n",
    "im_list = [os.path.join(root_path, 'ants', i) for i in os.listdir(root_path + 'ants')[:4]]\n",
    "im_list += [os.path.join(root_path, 'bees', i) for i in os.listdir(root_path + 'bees')[:5]]\n",
    "\n",
    "nrows = 3\n",
    "ncols = 3\n",
    "figsize = (8, 8)\n",
    "_, figs = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        figs[i][j].imshow(Image.open(im_list[nrows*i+j]))\n",
    "        figs[i][j].axes.get_xaxis().set_visible(False)\n",
    "        figs[i][j].axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们用封装在`custom_input.py`里的函数读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_label_dict, train_names, train_imgs, train_labels, train_examples = read('hymenoptera_data/train/', shuffle=True, batch_size=64)\n",
    "_, val_names, val_imgs, val_labels, val_examples = read('hymenoptera_data/val/', category_label_dict, train=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`tf-slim`中的网络定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim.python.slim.nets.resnet_v2 as resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slim_resnet(inputs, num_classes=2, is_training=True, scope='slim_resnet', reuse=None):\n",
    "    logits, endpts = resnet.resnet_v2_50(inputs, 10, is_training, reuse=reuse, scope=scope)\n",
    "    out = tf.squeeze(logits, [1, 2])\n",
    "    \n",
    "    return out, endpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "is_training_ph = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "with slim.arg_scope(resnet.resnet_arg_scope(weight_decay=0.0005)):\n",
    "    train_out, train_endpts = slim_resnet(train_imgs, 2, is_training_ph)\n",
    "    val_out, val_endpts = slim_resnet(val_imgs, 2, is_training_ph, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    train_loss = tf.losses.sparse_softmax_cross_entropy(labels=train_labels, logits=train_out, scope='train')\n",
    "    val_loss = tf.losses.sparse_softmax_cross_entropy(labels=val_labels, logits=val_out, scope='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('train'):\n",
    "        train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_out, axis=-1, output_type=tf.int32), train_labels), tf.float32))\n",
    "    with tf.name_scope('val'):\n",
    "        val_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(val_out, axis=-1, output_type=tf.int32), val_labels), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "opt = tf.train.MomentumOptimizer(lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = opt.minimize(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 恢复预训练模型参数\n",
    "\n",
    "- **Point1**\n",
    "\n",
    "首先我们注意到模型文件是`.ckpt`文件, 里面存储的模型参数都有自己的参数名称, 在我们恢复参数数值的过程中, `tensorflow`会按照参数名称进行加载. \n",
    "\n",
    "通常预训练模型的参数名和我们自己定义的参数名不完全相同, 但存在对应关系. 比如说`tf-slim`的`resnet50`模型的所有参数名称都有`'resnet_v2_50'`前缀, 而我们所有参数模型的前缀是`'slim_resnet'`.\n",
    "\n",
    "那么我们建立一个字典, 预训练模型的参数名称作为`key`, 参数作为`value`. 这样在恢复参数的过程中, `tensorflow`根据字典的`key`找到预训练模型中的参数数值, 根据`value`找到需要被恢复的参数变量, 这样就完成了参数恢复.\n",
    "\n",
    "- - -\n",
    "\n",
    "- **Point2**\n",
    "\n",
    "然后需要注意我们能够恢复哪些参数, 例如在这里最后一个分类层的参数, `imagenet`比赛中是`1000`分类, 所以它的最后一层是$2048\\times1000$的权重, 而在这个2分类问题中, 我们的参数是$2048\\times2$的, 所以不能进行恢复, 只能初始化再训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_path = 'pretrained_models/resnet_v2_50/model.ckpt'\n",
    "\n",
    "# 构建需要恢复的名称变量对应字典\n",
    "vars_to_restore = {}\n",
    "\n",
    "for var in tf.global_variables():\n",
    "    # 需要恢复的模型变量\n",
    "    if var in tf.model_variables():\n",
    "        \n",
    "        # 不需要恢复最后一层的参数\n",
    "        if 'logit' not in var.op.name:\n",
    "            \n",
    "            # 找到预训练模型中参数变量的名字\n",
    "            pretrained_model_var_name = var.op.name.replace('slim_resnet', 'resnet_v2_50')\n",
    "            \n",
    "            # 添加到对应字典中\n",
    "            vars_to_restore[pretrained_model_var_name] = var\n",
    "\n",
    "vars_to_init = list(filter(lambda var: var not in vars_to_restore.values(), tf.global_variables()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们来看看`vars_to_restore`和`vars_to_init`里面的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('vars_to_restore')\n",
    "print('-'*30)\n",
    "for i in range(10):\n",
    "    print(list(vars_to_restore.values())[i].name)\n",
    "print('-'*30)\n",
    "print('vars_to_init')\n",
    "print('-'*30)\n",
    "for i in range(10):\n",
    "    print(vars_to_init[i].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现, `bn`层的`moving_mean`和`moving_average`都是需要恢复的\n",
    "\n",
    "最后一个分类层(`logits`)和动量值是需要被训练的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立恢复变量的读取器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restorer = tf.train.Saver(vars_to_restore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 恢复模型变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restorer.restore(save_path=pretrained_model_path, sess=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么现在我们来用`graph`的`get_tensor_by_name`来看看恢复后参数的数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(tf.get_default_graph().get_tensor_by_name('slim_resnet/conv1/weights:0')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化其他变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.variables_initializer(vars_to_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = train_with_bn(sess, \n",
    "                              train_op, \n",
    "                              train_loss, \n",
    "                              train_acc, \n",
    "                              val_loss, \n",
    "                              val_acc, \n",
    "                              150, \n",
    "                              is_training_ph, \n",
    "                              train_examples=train_examples, \n",
    "                              val_examples=val_examples, \n",
    "                              train_batch=64, \n",
    "                              val_batch=128, \n",
    "                              train_log_step=10, \n",
    "                              val_log_step=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到, 最后验证集上达到了0.89的正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恢复模型后还可以仅仅训练最后一个分类层的参数而保持其他模型参数不变, 这只需要构建训练过程中`minimize`函数的帮助就可以实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_train = list(filter(lambda var: var not in vars_to_restore.values(), tf.trainable_variables()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = opt.minimize(train_loss, var_list=vars_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_init = list(filter(lambda var: var not in vars_to_restore.values(), tf.global_variables()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新加载模型变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "restorer.restore(sess, pretrained_model_path)\n",
    "sess.run(tf.variables_initializer(vars_to_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = train_with_bn(sess, \n",
    "                              train_op, \n",
    "                              train_loss, \n",
    "                              train_acc, \n",
    "                              val_loss, \n",
    "                              val_acc, \n",
    "                              150, \n",
    "                              is_training_ph, \n",
    "                              train_examples=train_examples, \n",
    "                              val_examples=val_examples, \n",
    "                              train_batch=64, \n",
    "                              val_batch=128, \n",
    "                              train_log_step=10, \n",
    "                              val_log_step=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来看看参数值是否变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(tf.get_default_graph().get_tensor_by_name('slim_resnet/conv1/weights:0')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样其实相当于把前面的卷积网络当作特征提取器, 然后用一个单层神经网络去训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为对比, 我们再来看看完全不使用预训练模型时的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = opt.minimize(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = train_with_bn(sess, \n",
    "                              train_op, \n",
    "                              train_loss, \n",
    "                              train_acc, \n",
    "                              val_loss, \n",
    "                              val_acc, \n",
    "                              150, \n",
    "                              is_training_ph, \n",
    "                              train_examples=train_examples, \n",
    "                              val_examples=val_examples, \n",
    "                              train_batch=64, \n",
    "                              val_batch=128, \n",
    "                              train_log_step=10, \n",
    "                              val_log_step=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的结果可以看到，使用预训练的模型能够非常快的达到 90% 左右的验证集准确率，而不使用预训练模型训练集在相同步长内都难以收敛，所以使用一个预训练的模型能够在较小的数据集上也取得一个非常好的效果，因为对于图片识别分类任务，最底层的卷积层识别的都是一些通用的特征，比如形状、纹理等等，所以对于很多图像分类、识别任务，都可以使用预训练的网络得到更好的结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
