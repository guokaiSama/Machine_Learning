{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "微软亚洲研究院的研究员设计了更深但结构更加简单的网络 ResNet，并且凭借这个网络子在 2015 年 ImageNet 比赛上大获全胜。\n",
    "\n",
    "ResNet 有效地解决了深度神经网络难以训练的问题，可以训练高达 1000 层的卷积网络。网络之所以难以训练，是因为存在着梯度消失的问题，离 loss 函数越远的层，在反向传播的时候，梯度越小，就越难以更新，随着层数的增加，这个现象越严重。之前有两种常见的方案来解决这个问题：\n",
    "\n",
    "1.按层训练，先训练比较浅的层，然后在不断增加层数，但是这种方法效果不是特别好，而且比较麻烦\n",
    "\n",
    "2.使用更宽的层，或者增加输出通道，而不加深网络的层数，这种结构往往得到的效果又不好\n",
    "\n",
    "ResNet 通过引入了跨层链接解决了梯度回传消失的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这就普通的网络连接跟跨层残差连接的对比图，使用普通的连接，上层的梯度必须要一层一层传回来，而是用残差连接，相当于中间有了一条更短的路，梯度能够从这条更短的路传回来，避免了梯度过小的情况。\n",
    "\n",
    "假设某层的输入是 x，期望输出是 H(x)， 如果我们直接把输入 x 传到输出作为初始结果，这就是一个更浅层的网络，更容易训练，而这个网络没有学会的部分，我们可以使用更深的网络 F(x) 去训练它，使得训练更加容易，最后希望拟合的结果就是 F(x) = H(x) - x，这就是一个残差的结构\n",
    "\n",
    "残差网络的结构就是上面这种残差块的堆叠，下面让我们来实现一个 residual block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "from utils import cifar10_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_imgs, train_labels, val_imgs, val_labels = cifar10_input.load_data(image_size=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 首先定义一个下采样函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(x, factor, scope=None):\n",
    "    if factor == 1:\n",
    "        return x\n",
    "    return slim.max_pool2d(x, [1, 1], factor, scope=scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义一个`residual_block`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, bottleneck_depth, out_depth, stride=1, scope='residual_block'):\n",
    "    in_depth = x.get_shape().as_list()[-1]\n",
    "    with tf.variable_scope(scope):\n",
    "        # 如果通道数没有改变,用下采样改变输入的大小\n",
    "        if in_depth == out_depth:\n",
    "            shortcut = subsample(x, stride, 'shortcut')\n",
    "        # 如果有变化, 用卷积改变输入的通道以及大小\n",
    "        else:\n",
    "            shortcut = slim.conv2d(x, out_depth, [1, 1], stride=stride, activation_fn=None, scope='shortcut')\n",
    "\n",
    "        residual = slim.conv2d(x, bottleneck_depth, [1, 1], stride=1, scope='conv1')\n",
    "        residual = slim.conv2d(residual, bottleneck_depth, 3, stride, scope='conv2')\n",
    "        residual = slim.conv2d(residual, out_depth, [1, 1], stride=1, activation_fn=None, scope='conv3')\n",
    "\n",
    "        # 相加操作\n",
    "        output = tf.nn.relu(shortcut + residual)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建`resnet`整体结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet(inputs, num_classes, reuse=None, is_training=None, verbose=False):\n",
    "    with tf.variable_scope('resnet', reuse=reuse):\n",
    "        net = inputs\n",
    "        \n",
    "        if verbose:\n",
    "            print('input: {}'.format(net.shape))\n",
    "        \n",
    "        with slim.arg_scope([slim.batch_norm], is_training=is_training):\n",
    "            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], padding='SAME'):\n",
    "                with tf.variable_scope('block1'):\n",
    "                    net = slim.conv2d(net, 32, [5, 5], stride=2, scope='conv_5x5')\n",
    "\n",
    "                    if verbose:\n",
    "                        print('block1: {}'.format(net.shape))\n",
    "                    \n",
    "                with tf.variable_scope('block2'):\n",
    "                    net = slim.max_pool2d(net, [3, 3], 2, scope='max_pool')\n",
    "                    net = residual_block(net, 32, 128, scope='residual_block1')\n",
    "                    net = residual_block(net, 32, 128, scope='residual_block2')\n",
    "\n",
    "                    if verbose:\n",
    "                        print('block2: {}'.format(net.shape))\n",
    "                    \n",
    "                with tf.variable_scope('block3'):\n",
    "                    net = residual_block(net, 64, 256, stride=2, scope='residual_block1')\n",
    "                    net = residual_block(net, 64, 256, scope='residual_block2')\n",
    "\n",
    "                    if verbose:\n",
    "                        print('block3: {}'.format(net.shape))\n",
    "                    \n",
    "                with tf.variable_scope('block4'):\n",
    "                    net = residual_block(net, 128, 512, stride=2, scope='residual_block1')\n",
    "                    net = residual_block(net, 128, 512, scope='residual_block2')\n",
    "\n",
    "                    if verbose:\n",
    "                        print('block4: {}'.format(net.shape))\n",
    "                \n",
    "                with tf.variable_scope('classification'):\n",
    "                    net = tf.reduce_mean(net, [1, 2], name='global_pool', keep_dims=True)\n",
    "                    net = slim.flatten(net, scope='flatten')\n",
    "                    net = slim.fully_connected(net, num_classes, activation_fn=None, normalizer_fn=None, scope='logit')\n",
    "\n",
    "                    if verbose:\n",
    "                        print('classification: {}'.format(net.shape))\n",
    "                    \n",
    "                return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm) as sc:\n",
    "    conv_scope = sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: (64, 96, 96, 3)\n",
      "block1: (64, 48, 48, 32)\n",
      "block2: (64, 24, 24, 128)\n",
      "block3: (64, 12, 12, 256)\n",
      "block4: (64, 6, 6, 512)\n",
      "WARNING:tensorflow:From <ipython-input-6-71873090ed65>:39: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "classification: (64, 10)\n"
     ]
    }
   ],
   "source": [
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "with slim.arg_scope(conv_scope):\n",
    "    train_out = resnet(train_imgs, 10, is_training=is_training, verbose=True)\n",
    "    val_out = resnet(val_imgs, 10, is_training=is_training, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    train_loss = tf.losses.sparse_softmax_cross_entropy(labels=train_labels, logits=train_out, scope='train')\n",
    "    val_loss = tf.losses.sparse_softmax_cross_entropy(labels=val_labels, logits=val_out, scope='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('train'):\n",
    "        train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_out, axis=-1, output_type=tf.int32), train_labels), tf.float32))\n",
    "    with tf.name_scope('val'):\n",
    "        val_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(val_out, axis=-1, output_type=tf.int32), val_labels), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "opt = tf.train.MomentumOptimizer(lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = opt.minimize(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.learning import train_with_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]: step 0 loss = 2.2766 acc = 0.1562 (0.0166 / batch)\n",
      "[val]: step 0 loss = 2.2960 acc = 0.1250\n",
      "[train]: step 1000 loss = 1.6274 acc = 0.4531 (0.0833 / batch)\n",
      "[train]: step 2000 loss = 2.4385 acc = 0.4375 (0.0828 / batch)\n",
      "[train]: step 3000 loss = 0.6490 acc = 0.7656 (0.0824 / batch)\n",
      "[train]: step 4000 loss = 1.1083 acc = 0.6562 (0.0826 / batch)\n",
      "[val]: step 4000 loss = 1.4970 acc = 0.5625\n",
      "[train]: step 5000 loss = 0.9370 acc = 0.7031 (0.0828 / batch)\n",
      "[train]: step 6000 loss = 0.7490 acc = 0.7344 (0.0826 / batch)\n",
      "[train]: step 7000 loss = 0.4196 acc = 0.8281 (0.0827 / batch)\n",
      "[train]: step 8000 loss = 0.4825 acc = 0.7969 (0.0827 / batch)\n",
      "[val]: step 8000 loss = 1.3932 acc = 0.7031\n",
      "[train]: step 9000 loss = 0.3133 acc = 0.8750 (0.0829 / batch)\n",
      "[train]: step 10000 loss = 0.3113 acc = 0.8906 (0.0827 / batch)\n",
      "[train]: step 11000 loss = 0.1614 acc = 0.9531 (0.0825 / batch)\n",
      "[train]: step 12000 loss = 0.1955 acc = 0.8906 (0.0827 / batch)\n",
      "[val]: step 12000 loss = 2.0709 acc = 0.6719\n",
      "[train]: step 13000 loss = 0.1915 acc = 0.9219 (0.0827 / batch)\n",
      "[train]: step 14000 loss = 0.6240 acc = 0.8125 (0.0829 / batch)\n",
      "[train]: step 15000 loss = 0.2221 acc = 0.8906 (0.0828 / batch)\n",
      "[train]: step 16000 loss = 0.0508 acc = 0.9844 (0.0828 / batch)\n",
      "[val]: step 16000 loss = 0.6743 acc = 0.7969\n",
      "[train]: step 17000 loss = 0.0040 acc = 1.0000 (0.0827 / batch)\n",
      "[train]: step 18000 loss = 0.0429 acc = 0.9688 (0.0828 / batch)\n",
      "[train]: step 19000 loss = 0.1135 acc = 0.9688 (0.0828 / batch)\n",
      "[train]: step 20000 loss = 0.0342 acc = 0.9844 (0.0829 / batch)\n",
      "[val]: step 20000 loss = 1.7708 acc = 0.6562\n",
      "-------------------------Over all Result-------------------------\n",
      "[TRAIN]: loss = 0.0389 acc = 0.9861\n",
      "[VAL]: loss = 1.4745 acc = 0.7565\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_with_bn(sess, train_op, train_loss, train_acc, val_loss, val_acc, 20000, is_training)\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
