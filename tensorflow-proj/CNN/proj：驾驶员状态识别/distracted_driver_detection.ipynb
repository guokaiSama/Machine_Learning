{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# 导入所有的包\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img1 = Image.open('./dataset/train/c0/img_12247.jpg')\n",
    "img2 = Image.open('./dataset/train/c1/img_100021.jpg')\n",
    "img3 = Image.open('./dataset/train/c2/img_100108.jpg')\n",
    "img4 = Image.open('./dataset/train/c3/img_100006.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "nrows = 2\n",
    "ncols = 2\n",
    "figsize = (10, 10)\n",
    "_, figs = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "figs[0, 0].imshow(img1)\n",
    "figs[0, 0].axes.set_xlabel('safe driving')\n",
    "figs[0, 0].axes.get_yaxis().set_visible(False)\n",
    "figs[0, 1].imshow(img2)\n",
    "figs[0, 1].axes.set_xlabel('texting right')\n",
    "figs[0, 1].axes.get_yaxis().set_visible(False)\n",
    "figs[1, 0].imshow(img3)\n",
    "figs[1, 0].axes.set_xlabel('talking on the phone right')\n",
    "figs[1, 0].axes.get_yaxis().set_visible(False)\n",
    "figs[1, 1].imshow(img4)\n",
    "figs[1, 1].axes.set_xlabel('texting left')\n",
    "figs[1, 1].axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入数据及预处理\n",
    "\n",
    "接下来读入数据,用 tf.data 方法可以非常方便地操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先从`dataset/driver_imgs_list.csv`中读取图片路径以及标签列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "image_dir = 'dataset/train_valid/'\n",
    "\n",
    "image_names = []\n",
    "image_labels = []\n",
    "\n",
    "with open('dataset/driver_imgs_list.csv') as f:\n",
    "    f_csv = csv.DictReader(f)\n",
    "    for row in f_csv:\n",
    "        classname = row['classname']\n",
    "        image_labels.append(int(classname[-1]))\n",
    "        image_names.append(os.path.join(image_dir, classname, row['img']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确定训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES_OF_TRAIN = 20000\n",
    "NUM_EXAMPLES_OF_VALID = len(image_names) - NUM_EXAMPLES_OF_TRAIN\n",
    "\n",
    "train_names, train_labels = image_names[:NUM_EXAMPLES_OF_TRAIN], image_labels[:NUM_EXAMPLES_OF_TRAIN]\n",
    "valid_names, valid_labels = image_names[NUM_EXAMPLES_OF_TRAIN:], image_labels[NUM_EXAMPLES_OF_TRAIN:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`tf.data`快速构造读取数据函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(names, labels, batch_size=None, num_epoch=None, shuffle=False, phase='train'):\n",
    "    def _read_img(name):\n",
    "        #TODO\n",
    "        # 给定图像名称tensor, 输出3维浮点值图像\n",
    "        img = None\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def _train_preprocess(img):\n",
    "        #TODO\n",
    "        # 对训练集图像预处理\n",
    "        # 例如resize到固定大小,翻转,调整对比度等等\n",
    "        train_img = None\n",
    "        \n",
    "        return train_img\n",
    "    \n",
    "    def _eval_preprocess(img):\n",
    "        #TODO\n",
    "        # 对验证集, 测试集图像预处理\n",
    "        # 例如resize到固定大小等等\n",
    "        eval_img = None\n",
    "        \n",
    "        return eval_img\n",
    "    \n",
    "    #TODO\n",
    "    # 构造图像名称 dataset\n",
    "    name_dataset = None\n",
    "    \n",
    "    #TODO\n",
    "    # 通过 map 函数调用 _read_img 构造图像 dataset\n",
    "    image_dataset = None\n",
    "    \n",
    "    if phase == 'train':\n",
    "        #TODO\n",
    "        # 通过 map 函数对训练集图像进行处理\n",
    "        image_dataset = None\n",
    "    else:\n",
    "        #TODO\n",
    "        # 通过 map 函数对验证集,测试集图像进行处理\n",
    "        image_dataset = None\n",
    "\n",
    "    #TODO\n",
    "    # 构造图像标签 dataset\n",
    "    label_dataset = None\n",
    "    \n",
    "    #TODO\n",
    "    # 将图像以及图像标签 dataset 通过 zip 合并成一个 dataset\n",
    "    dataset = None\n",
    "    \n",
    "    #TODO\n",
    "    # 设置 dataset 的 epoch\n",
    "    dataset = None\n",
    "    \n",
    "    #TODO\n",
    "    # 在需要 shuffle 时, 对 dataset 进行 shuffle\n",
    "    if shuffle:\n",
    "        dataset = None\n",
    "    \n",
    "    #TODO\n",
    "    # 在需要进行 batch 时, 对 dataset 进行 batch\n",
    "    if batch_size is not None:\n",
    "        dataset = None\n",
    "    \n",
    "    #TODO\n",
    "    # 构造 dataset 的迭代器\n",
    "    iterator = None\n",
    "\n",
    "    #TODO\n",
    "    # 获取 dataset 的元素\n",
    "    image, label = None\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "train_images, train_labels = read(train_names, train_labels, batch_size=batch_size, shuffle=True, phase='train')\n",
    "valid_images, valid_labels = read(valid_names, valid_labels, batch_size=batch_size, shuffle=False, phase='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 测试\n",
    "# =========这里不要修改=========\n",
    "import numpy as np\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    img_fetch, label_fetch = sess.run([train_images, train_labels])\n",
    "    assert img_fetch.ndim == 4, '图像维数 != 4'\n",
    "    assert label_fetch.ndim == 1, '标签维数 != 1'\n",
    "    plt.imshow(np.uint8(img_fetch[0]))\n",
    "    plt.title('Label: %d' % label_fetch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型\n",
    "\n",
    "读取数据完成后, 我们需要使用 CNN 构建从图像到预测 label 的模型, 可以自己构造模型, 也可以使用`TF-Slim`中预置的模型函数来构造, 第二种方式的好处是我们可以使用在 imagenet 比赛中训练好的与训练模型, 也就是之前提到的 fine-tune 策略.\n",
    "\n",
    "两种方式都可以, 下面你将可以使用自己的方式去构造这个分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(inputs, num_classes, is_training=False, scope='model', reuse=None):\n",
    "    '''\n",
    "    function to build forward model.\n",
    "    \n",
    "    Arguments:\n",
    "      inputs: 4维输入图像tensor.\n",
    "              \n",
    "      num_classes: 分类数.\n",
    "                   \n",
    "      is_training: 对使用 BN 层的神经网络非常有用, 你可以使用它或者不使用.\n",
    "                   \n",
    "      scope: 变量域的名称.\n",
    "             \n",
    "      reuse: 是否需要重用该变量域下的变量.\n",
    "      \n",
    "    Return:\n",
    "      output: 预测模型在使用最后一个 softmax 或者 sigmoid 之前的输出.\n",
    "    '''\n",
    "    #TODO\n",
    "    # 构造你自己的预测模型\n",
    "    output = None\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "train_out = model(train_images, num_classes=10, is_training=is_training)\n",
    "valid_out = model(valid_images, num_classes=10, is_training=is_training, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "# =========这里不要修改=========\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_fetch = sess.run(train_out, feed_dict={is_training: False})\n",
    "    assert output_fetch.ndim == 2, '网络输出维数 != 2'\n",
    "    assert output_fetch.shape[-1] == 10, '输出类别数 != 10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建 loss 函数,计算正确率和构造优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# 计算 loss\n",
    "\n",
    "train_loss = None\n",
    "valid_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# 计算概率\n",
    "\n",
    "train_prob = None\n",
    "valid_prob = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# 计算 accuracy\n",
    "\n",
    "train_acc = None\n",
    "valid_acc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# 定义优化方法\n",
    "\n",
    "opt = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你的预测模型中使用了 BN 层, 那么完成 **下面两条** 内容, 否则跳过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# 得到所有 bn 更新算子\n",
    "\n",
    "update_ops = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# 构造训练方法\n",
    "\n",
    "train_op = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你的模型没有使用 BN 层, 完成 **下面一条** 内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# 构造训练 op\n",
    "\n",
    "train_op = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "# =========这里不要修改=========\n",
    "\n",
    "grads = tf.gradients(train_loss, tf.trainable_variables())\n",
    "assert None not in grads, 'loss 对某参数不可微分'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "\n",
    "当构建好了前面的过程,我们就可以开始训练了,整个训练的框架已经写好,需要做的就是在需要的地方补充完整的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练的 epochs 数目\n",
    "max_epoch = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是训练的代码,你只需要填写其中的**todo**代码\n",
    "\n",
    "**注意:如果发现显存超过限制,可以改小 batch size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_op, train_loss, train_acc, max_epoch, \n",
    "          is_training=None, valid_loss=None, valid_acc=None, save_path=None, pretrained_model=None):\n",
    "    '''\n",
    "    训练函数.\n",
    "    \n",
    "    Arguments:\n",
    "      train_op: 训练 op\n",
    "      \n",
    "      train_loss: 作用在训练集上的 loss\n",
    "      \n",
    "      train_acc: 作用在训练集上的 accuracy\n",
    "      \n",
    "      max_epoch: 训练最大步长\n",
    "      \n",
    "      is_training: 使用 BN 层时的 placeholder\n",
    "      \n",
    "      valid_loss: 作用在验证集上的 loss\n",
    "      \n",
    "      valid_acc: 作用在验证集上的 accuracy\n",
    "      \n",
    "      save_path: 希望保存的模型路径\n",
    "      \n",
    "      pretrained_model: 希望使用的与训练模型路径\n",
    "    '''\n",
    "    # 开始训练\n",
    "    freq_print = NUM_EXAMPLES_OF_TRAIN // 10\n",
    "    \n",
    "    log_dir = 'log'\n",
    "\n",
    "    if pretrained_model is not None:\n",
    "        #TODO\n",
    "        # 找到所有需要 finetune 的变量\n",
    "        vars_to_finetune = None\n",
    "\n",
    "        vars_to_init = None\n",
    "        \n",
    "        #TODO\n",
    "        # 生成一个对上面变量的加载器\n",
    "        restorer = None\n",
    "    \n",
    "    if save_path is not None:\n",
    "        #TODO\n",
    "        # 生成一个空参数的变量保存器\n",
    "        saver = None\n",
    "    \n",
    "    sess = tf.Session()\n",
    "        \n",
    "    graph_writer = tf.summary.FileWriter(log_dir, graph=sess.graph)\n",
    "\n",
    "    if pretrained_model is not None:\n",
    "        #TODO\n",
    "        # 使用加载器恢复变量的数值,并初始化其他变量\n",
    "        pass\n",
    "    else:\n",
    "        #TODO\n",
    "        # 初始化所有变量\n",
    "        pass\n",
    "\n",
    "    curr_epoch = 0\n",
    "    curr_step = 0\n",
    "    curr_valid_step = 0\n",
    "\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "\n",
    "    metric_log = dict()\n",
    "    metric_log['train_loss'] = list()\n",
    "    metric_log['train_acc'] = list()\n",
    "    metric_log['valid_loss'] = list()\n",
    "    metric_log['valid_acc'] = list()\n",
    "\n",
    "    while curr_epoch < max_epoch:\n",
    "        if is_training is not None:\n",
    "            # TODO\n",
    "            # 运行训练 op 前, 对 bool 型占位符 is_training 进行赋值\n",
    "            train_feed_dict = {is_training: None}\n",
    "\n",
    "            # TODO\n",
    "            # 运行训练 op, 同时输出当前训练 batch 上的 loss 和 accuracy\n",
    "            pass\n",
    "        else:\n",
    "            #TODO\n",
    "            # 运行训练 op, 同时输出当前训练 batch 上的 loss 和 accuracy\n",
    "            _, batch_loss, batch_acc = None, None, None\n",
    "\n",
    "        running_loss += batch_loss\n",
    "        running_acc += batch_acc\n",
    "\n",
    "        curr_step += batch_size\n",
    "\n",
    "        if curr_step // freq_print > (curr_step - batch_size) // freq_print:\n",
    "            print('[{}]/[{}], train loss: {:.3f}, train acc: {:.3f}'.format(\n",
    "                curr_step, NUM_EXAMPLES_OF_TRAIN, running_loss / curr_step * batch_size, \n",
    "                running_acc / curr_step * batch_size))\n",
    "\n",
    "        if curr_step > NUM_EXAMPLES_OF_TRAIN:\n",
    "            # 当前 epoch 结束\n",
    "            curr_epoch += 1\n",
    "\n",
    "            metric_log['train_loss'].append(running_loss / curr_step * batch_size)\n",
    "            metric_log['train_acc'].append(running_acc / curr_step * batch_size)\n",
    "\n",
    "            if (valid_loss is not None and valid_acc is not None):\n",
    "                running_loss = 0\n",
    "                running_acc = 0\n",
    "                \n",
    "                if is_training is not None:\n",
    "                    # 使用 BN\n",
    "                    # TODO\n",
    "                    # 计算验证集上所有样本的 loss 和 accuracy, 对 bool 型占位符 is_training 进行赋值\n",
    "                    eval_feed_dict = {is_training: None}\n",
    "\n",
    "                    while curr_valid_step < NUM_EXAMPLES_OF_VALID:\n",
    "                        # TODO\n",
    "                        # 输出当前验证 batch 上的 loss 和 accuracy\n",
    "\n",
    "                        batch_loss, batch_acc = None, None\n",
    "                        \n",
    "                        running_loss += batch_loss\n",
    "                        running_acc += batch_acc\n",
    "                        curr_valid_step += batch_size\n",
    "                else:\n",
    "                    # 不使用 BN\n",
    "                    # TODO\n",
    "                    # 计算验证集上所有样本的 loss 和 accuracy, 对 bool 型占位符 is_training 进行赋值\n",
    "\n",
    "                    while curr_valid_step < NUM_EXAMPLES_OF_VALID:\n",
    "                        # TODO\n",
    "                        # 输出当前验证 batch 上的 loss 和 accuracy\n",
    "\n",
    "                        batch_loss, batch_acc = None, None\n",
    "                        running_loss += batch_loss\n",
    "                        running_acc += batch_acc\n",
    "                        curr_valid_step += batch_size\n",
    "\n",
    "                metric_log['valid_loss'].append(running_loss / curr_valid_step * batch_size)\n",
    "                metric_log['valid_acc'].append(running_acc / curr_valid_step * batch_size)\n",
    "\n",
    "                curr_valid_step = curr_valid_step % NUM_EXAMPLES_OF_VALID\n",
    "\n",
    "                print_str = 'epoch: {}, train loss: {:.3f}, train acc: {:.3f}, valid loss: {:.3f}, valid acc: {:.3f}'.format(\n",
    "            curr_epoch, metric_log['train_loss'][-1], metric_log['train_acc'][-1], \n",
    "            metric_log['valid_loss'][-1], metric_log['valid_acc'][-1])\n",
    "\n",
    "            else:\n",
    "                print_str = 'epoch: {}, train loss: {:.3f}, train acc: {:.3f}'.format(curr_epoch, \n",
    "                                metric_log['train_loss'][-1], metric_log['train_acc'][-1])\n",
    "\n",
    "            print(print_str)\n",
    "            print()\n",
    "\n",
    "            curr_step = curr_step % NUM_EXAMPLES_OF_TRAIN\n",
    "            running_loss = 0\n",
    "            running_acc = 0\n",
    "            \n",
    "    \n",
    "    # =======不要修改这里的内容========\n",
    "    # 保存模型\n",
    "    if save_path is not None:\n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "        saved_path = saver.save(sess, '%s/model.ckpt' % save_path)\n",
    "        print('model saved to %s' % saved_path)\n",
    "        \n",
    "    sess.close()\n",
    "    \n",
    "    # 可视化\n",
    "    if valid_loss is not None and valid_acc is not None:\n",
    "        nrows = 2\n",
    "        ncols = 2\n",
    "        figsize = (10, 10)\n",
    "        _, figs = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        figs[0, 0].plot(metric_log['train_loss'])\n",
    "        figs[0, 0].axes.set_xlabel('train loss')\n",
    "        figs[0, 1].plot(metric_log['train_acc'])\n",
    "        figs[0, 1].axes.set_xlabel('train acc')\n",
    "        figs[1, 0].plot(metric_log['valid_loss'])\n",
    "        figs[1, 0].axes.set_xlabel('valid loss')\n",
    "        figs[1, 1].plot(metric_log['valid_acc'])\n",
    "        figs[1, 1].axes.set_xlabel('valid acc')\n",
    "    else:\n",
    "        nrows = 1\n",
    "        ncols = 2\n",
    "        figsize= (10, 5)\n",
    "        _, figs = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        figs[0].plot(metric_log['train_loss'])\n",
    "        figs[0].axes.set_xlabel('train loss')\n",
    "        figs[1].plot(metric_log['train_acc'])\n",
    "        figs[1].axes.set_xlabel('train acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(train_op, train_loss, train_acc, 1, is_training, valid_loss, valid_acc, save_path='tmp', pretrained_model='pretrained_models/resnet_v2_50/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的结果，我们已经将模型跑起来了，那么你的下一个工作就是不断地调参，将验证集的准确率调高，具体的调参方式在课上已经讲过了，比如增加更多的数据增强，或者使用学习率衰减等等，可以通过输出的结果和画出的图像来判断最后模型的好坏\n",
    "\n",
    "**注意：可能在调参的过程中会出现显存超过限制，这是因为前面一个模型没有释放，所以最好每次调参都重新启动这个 notebook**\n",
    "\n",
    "当你调完参数之后，得到了一个最优的模型训练方式，那么可以重新训练模型，因为前面我们将数据集拆分成了训练集和验证集两个部分帮助我们调参，现在我们使用完整的数据集重新进行训练以便结果的提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清空当前图\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from utils import read_test, predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names, test_images = read_test('dataset/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = predict_result(test_images, test_names, 'tmp/model.ckpt', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {}\n",
    "for i in range(10):\n",
    "    idx_to_class[i] = 'c%d' % i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.columns = [['img'] + [i for i in idx_to_class.values()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过运行上面的程序，我们能够在根目录下得到一个提交的csv文件，叫做`submission.csv`，最后只需要将这个文件提交到[kaggle的比赛界面](https://www.kaggle.com/c/state-farm-distracted-driver-detection/leaderboard)即可获得比赛的成绩。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
