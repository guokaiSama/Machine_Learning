{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG\n",
    "vggNet 是第一个真正意义上的深层网络结构，其是 ImageNet2014年的冠军\n",
    "vgg 几乎全部使用 3 x 3 的卷积核以及 2 x 2 的池化层，使用小的卷积核进行多层的堆叠和一个大的卷积核的感受野是相同的，同时小的卷积核还能减少参数，同时可以有更深的结构。\n",
    "\n",
    "vgg 的一个关键就是使用很多层 3 x 3 的卷积然后再使用一个最大池化层，这个模块被使用了很多次，下面我们照着这个结构来写一写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from utils import cifar10_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们定义一个批次有64个样本\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 先下载数据\n",
    "#链接：https://pan.baidu.com/s/1fbThmv8LDhLRemA62m3nlQ \n",
    "#提取码：j1d8 \n",
    "# 解压文件，建立目录'./cifar10_data/cifar-10-batches-bin/'\n",
    "train_imgs, train_labels, val_imgs, val_labels = cifar10_input.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.layers import conv, max_pool, fc\n",
    "from utils.learning import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `vgg`是一个不断堆叠的网络结构, 我们从它重复的最小单元写起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(inputs, num_convs, out_depth, scope='vgg_block', reuse=None):\n",
    "    \"\"\"构建vgg_block.\n",
    "    \n",
    "    一个 vgg_block 由`num_convs`个卷积层和一个最大值池化层构成.\n",
    "    \n",
    "    Args:\n",
    "        inputs: 输入\n",
    "        num_convs: 这一个block里卷积层的个数\n",
    "        out_depth: 每一个卷积层的卷积核个数\n",
    "        scope: 变量域名\n",
    "        reuse: 是否复用\n",
    "    \"\"\"\n",
    "    in_depth = inputs.get_shape().as_list()[-1]\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse) as sc:\n",
    "        net = inputs\n",
    "        # 循环定义`num_convs`个卷积层\n",
    "        for i in range(num_convs):\n",
    "            net = conv(net, ksize=[3, 3], out_depth=out_depth, strides=[1, 1], padding='SAME', scope='conv%d' % i, reuse=reuse)\n",
    "        net = max_pool(net, [2, 2], [2, 2], name='pool')\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 然后我们把很多个不同的`vgg_block`堆叠在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_stack(inputs, num_convs, out_depths, scope='vgg_stack', reuse=None):\n",
    "    \"\"\"构建vgg_stack.\n",
    "    \n",
    "    一个 vgg_stack 将若干个不同的`vgg_block`进行`stack`(堆叠)\n",
    "    \n",
    "    Args:\n",
    "        inputs: 输入\n",
    "        num_convs: 每一个block里卷积层的个数, 列表. 如`[1, 2, 3]`\n",
    "        out_depths: 每一个block的卷积核个数, 列表, 如`[64, 128, 256]`\n",
    "        scope: 变量域名\n",
    "        reuse: 是否复用\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse) as sc:\n",
    "        net = inputs\n",
    "        for i, (n, d) in enumerate(zip(num_convs, out_depths)):\n",
    "            net = vgg_block(net, n, d, scope='block%d' % i)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 最后我们在通过几个全连接层将`vgg`搭建完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(inputs, num_convs, out_depths, num_outputs, scope='vgg', reuse=None):\n",
    "    \"\"\"构建vgg.\n",
    "    \n",
    "    一个 vgg 先经过`vgg_stack`后再连接两个全连接层.\n",
    "    \n",
    "    Args:\n",
    "        inputs: 输入\n",
    "        num_convs: 每一个 vgg_block 的卷积层的个数\n",
    "        out_depths: 每一个 vgg_block 卷积核个数\n",
    "        num_outputs: 最后输出向量的维数\n",
    "        scope: 变量域名\n",
    "        reuse: 是否复用\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse) as sc:\n",
    "        net = vgg_stack(inputs, num_convs, out_depths)\n",
    "        with tf.variable_scope('classification'):\n",
    "            net = tf.reshape(net, (batch_size, -1))\n",
    "            net = fc(net, 100, scope='fc1')\n",
    "            net = fc(net, num_outputs, act=tf.identity, scope='classification')\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out = vgg(train_imgs, (1, 1, 2, 2, 2), (64, 128, 256, 512, 512), 10)\n",
    "# 复用上面的参数\n",
    "val_out = vgg(val_imgs, (1, 1, 2, 2, 2), (64, 128, 256, 512, 512), 10, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义损失计算`op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    train_loss = tf.losses.sparse_softmax_cross_entropy(labels=train_labels, logits=train_out, scope='train')\n",
    "    val_loss = tf.losses.sparse_softmax_cross_entropy(labels=val_labels, logits=val_out, scope='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义正确率计算`op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('train'):\n",
    "        train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_out, axis=-1, output_type=tf.int32), train_labels), tf.float32))\n",
    "    with tf.name_scope('val'):\n",
    "        val_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(val_out, axis=-1, output_type=tf.int32), val_labels), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建训练`op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "opt = tf.train.MomentumOptimizer(lr, momentum=0.9)\n",
    "train_op = opt.minimize(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]: step 0 loss = 19.6095 acc = 0.0938 (0.0088 / batch)\n",
      "[val]: step 0 loss = 14.9871 acc = 0.0938\n",
      "[train]: step 1000 loss = 1.3572 acc = 0.5312 (0.0261 / batch)\n",
      "[train]: step 2000 loss = 1.0598 acc = 0.6719 (0.0244 / batch)\n",
      "[train]: step 3000 loss = 0.7525 acc = 0.7188 (0.0249 / batch)\n",
      "[train]: step 4000 loss = 0.5278 acc = 0.8594 (0.0256 / batch)\n",
      "[val]: step 4000 loss = 0.8461 acc = 0.7188\n",
      "[train]: step 5000 loss = 0.5448 acc = 0.8281 (0.0257 / batch)\n",
      "[train]: step 6000 loss = 0.4474 acc = 0.8281 (0.0260 / batch)\n",
      "[train]: step 7000 loss = 0.4204 acc = 0.8750 (0.0256 / batch)\n",
      "[train]: step 8000 loss = 0.2745 acc = 0.9219 (0.0267 / batch)\n",
      "[val]: step 8000 loss = 0.8315 acc = 0.7656\n",
      "[train]: step 9000 loss = 0.2414 acc = 0.9062 (0.0271 / batch)\n",
      "[train]: step 10000 loss = 0.0605 acc = 0.9844 (0.0264 / batch)\n",
      "[train]: step 11000 loss = 0.1731 acc = 0.9688 (0.0264 / batch)\n",
      "[train]: step 12000 loss = 0.1011 acc = 0.9531 (0.0252 / batch)\n",
      "[val]: step 12000 loss = 0.7041 acc = 0.7969\n",
      "[train]: step 13000 loss = 0.1321 acc = 0.9375 (0.0245 / batch)\n",
      "[train]: step 14000 loss = 0.1107 acc = 0.9531 (0.0239 / batch)\n",
      "[train]: step 15000 loss = 0.0492 acc = 0.9844 (0.0249 / batch)\n",
      "[train]: step 16000 loss = 0.0964 acc = 0.9688 (0.0246 / batch)\n",
      "[val]: step 16000 loss = 1.0701 acc = 0.7344\n",
      "[train]: step 17000 loss = 0.2029 acc = 0.9219 (0.0258 / batch)\n",
      "[train]: step 18000 loss = 0.0420 acc = 0.9688 (0.0226 / batch)\n",
      "[train]: step 19000 loss = 0.0393 acc = 0.9844 (0.0247 / batch)\n",
      "[train]: step 20000 loss = 0.0881 acc = 0.9531 (0.0245 / batch)\n",
      "[val]: step 20000 loss = 1.6487 acc = 0.7188\n",
      "-------------------------Over all Result-------------------------\n",
      "[TRAIN]: loss = 0.0580 acc = 0.9819\n",
      "[VAL]: loss = 1.2700 acc = 0.7629\n"
     ]
    }
   ],
   "source": [
    "train(train_op, train_loss, train_acc, val_loss, val_acc, 20000, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到, 20000步的训练后, `VGG`在训练集和测试集分别达到了`0.97`和`0.75`的准确率"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
