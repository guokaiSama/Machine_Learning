{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化\n",
    "前面我们讲了数据增强和 dropout，而在实际使用中，现在的网络往往不使用 dropout，而是用另外一个技术，叫正则化。\n",
    "\n",
    "正则化是机器学习中提出来的一种方法，有 L1 和 L2 正则化，目前使用较多的是 L2 正则化，引入正则化相当于在 loss 函数上面加上一项，比如\n",
    "\n",
    "$$\n",
    "f = loss + \\lambda \\sum_{p \\in params} ||p||_2^2\n",
    "$$\n",
    "\n",
    "就是在 loss 的基础上加上了参数的二范数作为一个正则化，我们在训练网络的时候，不仅要最小化 loss 函数，同时还要最小化参数的二范数，也就是说我们会对参数做一些限制，不让它变得太大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们对新的损失函数 f 求导进行梯度下降，就有\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial p_j} = \\frac{\\partial loss}{\\partial p_j} + 2 \\lambda p_j\n",
    "$$\n",
    "\n",
    "那么在更新参数的时候就有\n",
    "\n",
    "$$\n",
    "p_j \\rightarrow p_j - \\eta (\\frac{\\partial loss}{\\partial p_j} + 2 \\lambda p_j) = p_j - \\eta \\frac{\\partial loss}{\\partial p_j} - 2 \\eta \\lambda p_j \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到 $p_j - \\eta \\frac{\\partial loss}{\\partial p_j}$ 和没加正则项要更新的部分一样，而后面的 $2\\eta \\lambda p_j$ 就是正则项的影响，可以看到加完正则项之后会对参数做更大程度的更新，这也被称为权重衰减(weight decay)，在`tf-slim`中正则项可以通过`slim.arg_scope`和`slim.regularizers`来实现, 因为卷积层,全连接层都具有参数`weight_regularzier`，因此我们使用`slim.arg_scope([slim.conv2d, slim.fully_connected], weight_regularzier=slim.regularizers.l2_regularizer(weight_decay=0.0001))`就可以实现所有卷积层的权重`L2`模衰减\n",
    "\n",
    "注意正则项的系数的大小非常重要，如果太大，会极大的抑制参数的更新，导致欠拟合，如果太小，那么正则项这个部分基本没有贡献，所以选择一个合适的权重衰减系数非常重要，这个需要根据具体的情况去尝试，初步尝试可以使用 `1e-4` 或者 `1e-3` \n",
    "\n",
    "下面我们在训练 cifar 10 中添加正则项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from utils import cifar10_input\n",
    "from utils.resnet import resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs, train_labels, val_imgs, val_labels = cifar10_input.load_data(image_size=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给所有的`slim.conv2d`和`slim.fully_connected`添加默认权重衰减, 用`slim.arg_scope`统一定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm):\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected], weights_regularizer = slim.regularizers.l2_regularizer(1e-4)) as sc:\n",
    "        conv_scope = sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: (64, 96, 96, 3)\n",
      "block1: (64, 48, 48, 32)\n",
      "block2: (64, 24, 24, 128)\n",
      "block3: (64, 12, 12, 256)\n",
      "block4: (64, 6, 6, 512)\n",
      "WARNING:tensorflow:From /home/dgx/gklearn/CNN/utils/resnet.py:72: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "classification: (64, 10)\n"
     ]
    }
   ],
   "source": [
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "with slim.arg_scope(conv_scope):\n",
    "    train_out = resnet(train_imgs, 10, is_training=is_training, verbose=True)\n",
    "    val_out = resnet(val_imgs, 10, is_training=is_training, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    train_loss = tf.losses.sparse_softmax_cross_entropy(labels=train_labels, logits=train_out, scope='train')\n",
    "    val_loss = tf.losses.sparse_softmax_cross_entropy(labels=val_labels, logits=val_out, scope='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('train'):\n",
    "        train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_out, axis=-1, output_type=tf.int32), train_labels), tf.float32))\n",
    "    with tf.name_scope('val'):\n",
    "        val_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(val_out, axis=-1, output_type=tf.int32), val_labels), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "opt = tf.train.MomentumOptimizer(lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = opt.minimize(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.learning import train_with_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]: step 0 loss = 2.3034 acc = 0.1094 (0.0171 / batch)\n",
      "[val]: step 0 loss = 2.3041 acc = 0.0781\n",
      "[train]: step 1000 loss = 1.5135 acc = 0.4688 (0.0841 / batch)\n",
      "[train]: step 2000 loss = 1.0731 acc = 0.6875 (0.0834 / batch)\n",
      "[train]: step 3000 loss = 1.3064 acc = 0.6406 (0.0834 / batch)\n",
      "[train]: step 4000 loss = 0.7001 acc = 0.7656 (0.0835 / batch)\n",
      "[val]: step 4000 loss = 1.2375 acc = 0.5938\n",
      "[train]: step 5000 loss = 0.9213 acc = 0.7188 (0.0836 / batch)\n",
      "[train]: step 6000 loss = 0.4007 acc = 0.8438 (0.0841 / batch)\n",
      "[train]: step 7000 loss = 0.8119 acc = 0.7500 (0.0844 / batch)\n",
      "[train]: step 8000 loss = 0.6393 acc = 0.8438 (0.0821 / batch)\n",
      "[val]: step 8000 loss = 1.4887 acc = 0.6719\n",
      "[train]: step 9000 loss = 0.2326 acc = 0.8906 (0.0846 / batch)\n",
      "[train]: step 10000 loss = 0.5696 acc = 0.8594 (0.0846 / batch)\n",
      "[train]: step 11000 loss = 0.6103 acc = 0.8125 (0.0849 / batch)\n",
      "[train]: step 12000 loss = 0.1505 acc = 0.9375 (0.0852 / batch)\n",
      "[val]: step 12000 loss = 0.8726 acc = 0.6875\n",
      "[train]: step 13000 loss = 0.1814 acc = 0.9375 (0.0841 / batch)\n",
      "[train]: step 14000 loss = 0.0228 acc = 1.0000 (0.0834 / batch)\n",
      "[train]: step 15000 loss = 0.0962 acc = 0.9688 (0.0844 / batch)\n",
      "[train]: step 16000 loss = 0.1047 acc = 0.9531 (0.0846 / batch)\n",
      "[val]: step 16000 loss = 0.8923 acc = 0.8281\n",
      "[train]: step 17000 loss = 0.3937 acc = 0.8906 (0.0842 / batch)\n",
      "[train]: step 18000 loss = 0.0751 acc = 0.9688 (0.0846 / batch)\n",
      "[train]: step 19000 loss = 0.1634 acc = 0.9688 (0.0846 / batch)\n",
      "[train]: step 20000 loss = 0.0212 acc = 0.9844 (0.0845 / batch)\n",
      "[val]: step 20000 loss = 1.2527 acc = 0.7656\n",
      "-------------------------Over all Result-------------------------\n",
      "[TRAIN]: loss = 0.0269 acc = 0.9910\n",
      "[VAL]: loss = 1.2222 acc = 0.7700\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "_ = train_with_bn(sess, train_op, train_loss, train_acc, val_loss, val_acc, 20000, is_training)\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
