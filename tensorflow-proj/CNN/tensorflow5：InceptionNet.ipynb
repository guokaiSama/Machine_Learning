{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InceptionNet\n",
    "InceptionNet是 2014 年 ImageNet 比赛的冠军，这是 Google 的研究人员提出的网络结构(所以也叫做`GoogLeNet`)，在当时取得了非常大的影响，因为网络的结构变得前所未有，它颠覆了大家对卷积网络的串联的印象和固定做法，采用了一种非常有效的 inception 模块，得到了比 VGG 更深的网络结构，但是却比 VGG 的参数更少，因为其去掉了后面的全连接层，所以参数大大减少，同时有了很高的计算效率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception 模块\n",
    "在上面的网络中，我们看到了多个四个并行卷积的层，这些四个卷积并行的层就是 inception 模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个 inception 模块的四个并行线路如下：\n",
    "1.一个 1 x 1 的卷积，一个小的感受野进行卷积提取特征\n",
    "2.一个 1 x 1 的卷积加上一个 3 x 3 的卷积，1 x 1 的卷积降低输入的特征通道，减少参数计算量，然后接一个 3 x 3 的卷积做一个较大感受野的卷积\n",
    "3.一个 1 x 1 的卷积加上一个 5 x 5 的卷积，作用和第二个一样\n",
    "4.一个 3 x 3 的最大池化加上 1 x 1 的卷积，最大池化改变输入的特征排列，1 x 1 的卷积进行特征提取\n",
    "\n",
    "最后将四个并行线路得到的特征在通道这个维度上拼接在一起，下面我们可以实现一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "from utils import cifar10_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_imgs, train_labels, val_imgs, val_labels = cifar10_input.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用`tf-slim`构建网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception(x, d0_1, d1_1, d1_3, d2_1, d2_5, d3_1, scope='inception', reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 我们把`slim.conv2d`,`slim.max_pool2d`的默认参数放在`slim`的参数域里\n",
    "        with slim.arg_scope([slim.conv2d, slim.max_pool2d], stride=1, padding='SAME'):\n",
    "            # 第一个分支\n",
    "            with tf.variable_scope('branch0'):\n",
    "                branch_0 = slim.conv2d(x, d0_1, [1, 1], scope='conv_1x1')\n",
    "                \n",
    "            # 第二个分支    \n",
    "            with tf.variable_scope('branch1'):\n",
    "                branch_1 = slim.conv2d(x, d1_1, [1, 1], scope='conv_1x1')\n",
    "                branch_1 = slim.conv2d(branch_1, d1_3, [3, 3], scope='conv_3x3')\n",
    "                \n",
    "            # 第三个分支\n",
    "            with tf.variable_scope('branch2'):\n",
    "                branch_2 = slim.conv2d(x, d2_1, [1, 1], scope='conv_1x1')\n",
    "                branch_2 = slim.conv2d(branch_2, d2_1, [5, 5], scope='conv_5x5')\n",
    "                \n",
    "            # 第四个分支\n",
    "            with tf.variable_scope('branch3'):\n",
    "                branch_3 = slim.max_pool2d(x, [3, 3], scope='max_pool')\n",
    "                branch_3 = slim.conv2d(branch_3, d3_1, [1, 1], scope='conv_1x1')\n",
    "                \n",
    "            # 连接\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], axis=-1)\n",
    "            \n",
    "            return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`inception`模块去构建整个`googlenet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def googlenet(inputs, num_classes, reuse=None, is_training=None, verbose=False):\n",
    "    with tf.variable_scope('googlenet', reuse=reuse):\n",
    "        # 给`batch_norm`的`is_training`参数设定默认值.\n",
    "        # `batch_norm`和`is_training`密切相关, 当`is_trainig=True`时, \n",
    "        # 它使用的是一个`batch`数据的移动平均,方差值\n",
    "        # 当`is_training=True`时, 它使用的是固定值\n",
    "        with slim.arg_scope([slim.batch_norm], is_training=is_training):\n",
    "            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], padding='SAME', stride=1):\n",
    "                net = inputs\n",
    "                with tf.variable_scope('block1'):\n",
    "                    net = slim.conv2d(net, 64, [5, 5], stride=2, scope='conv_5x5')\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print('block1 output: {}'.format(net.shape))\n",
    "                        \n",
    "                with tf.variable_scope('block2'):\n",
    "                    net = slim.conv2d(net, 64, [1, 1], scope='conv_1x1')\n",
    "                    net = slim.conv2d(net, 192, [3, 3], scope='conv_3x3')\n",
    "                    net = slim.max_pool2d(net, [3, 3], stride=2, scope='max_pool')\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print('block2 output: {}'.format(net.shape))\n",
    "                        \n",
    "                with tf.variable_scope('block3'):\n",
    "                    net = inception(net, 64, 96, 128, 16, 32, 32, scope='inception_1')\n",
    "                    net = inception(net, 128, 128, 192, 32, 96, 64, scope='inception_2')\n",
    "                    net = slim.max_pool2d(net, [3, 3], stride=2, scope='max_pool')\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print('block3 output: {}'.format(net.shape))\n",
    "                        \n",
    "                with tf.variable_scope('block4'):\n",
    "                    net = inception(net, 192, 96, 208, 16, 48, 64, scope='inception_1')\n",
    "                    net = inception(net, 160, 112, 224, 24, 64, 64, scope='inception_2')\n",
    "                    net = inception(net, 128, 128, 256, 24, 64, 64, scope='inception_3')\n",
    "                    net = inception(net, 112, 144, 288, 24, 64, 64, scope='inception_4')\n",
    "                    net = inception(net, 256, 160, 320, 32, 128, 128, scope='inception_5')\n",
    "                    net = slim.max_pool2d(net, [3, 3], stride=2, scope='max_pool')\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print('block4 output: {}'.format(net.shape))\n",
    "                        \n",
    "                with tf.variable_scope('block5'):\n",
    "                    net = inception(net, 256, 160, 320, 32, 128, 128, scope='inception1')\n",
    "                    net = inception(net, 384, 182, 384, 48, 128, 128, scope='inception2')\n",
    "                    net = slim.avg_pool2d(net, [2, 2], stride=2, scope='avg_pool')\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print('block5 output: {}'.format(net.shape))\n",
    "                        \n",
    "                with tf.variable_scope('classification'):\n",
    "                    net = slim.flatten(net)\n",
    "                    net = slim.fully_connected(net, num_classes, activation_fn=None, normalizer_fn=None, scope='logit')\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print('classification output: {}'.format(net.shape))\n",
    "                    \n",
    "                return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给卷积层设置默认的激活函数和`batch_norm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm) as sc:\n",
    "    conv_scope = sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练的过程中, 所有`bn层`的`mean`和`variance`使用的是当前`batch`和之前`batch`的移动平均值\n",
    "\n",
    "而在预测的时候, 我们使用`bn层`本身的`mean`和`variance`.\n",
    "\n",
    "也就是说在训练和预测的时候, `bn层`是不同的\n",
    "\n",
    "所以训练集和验证集在`bn层`的参数不同, 用一个`placeholder`表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(tf.bool, name='is_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1 output: (64, 16, 16, 64)\n",
      "block2 output: (64, 8, 8, 192)\n",
      "block3 output: (64, 4, 4, 416)\n",
      "block4 output: (64, 2, 2, 736)\n",
      "block5 output: (64, 1, 1, 944)\n",
      "classification output: (64, 10)\n"
     ]
    }
   ],
   "source": [
    "with slim.arg_scope(conv_scope):\n",
    "    train_out = googlenet(train_imgs, 10, is_training=is_training, verbose=True)\n",
    "    val_out = googlenet(val_imgs, 10, is_training=is_training, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    train_loss = tf.losses.sparse_softmax_cross_entropy(labels=train_labels, logits=train_out, scope='train')\n",
    "    val_loss = tf.losses.sparse_softmax_cross_entropy(labels=val_labels, logits=val_out, scope='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('train'):\n",
    "        train_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(train_out, axis=-1, output_type=tf.int32), train_labels), tf.float32))\n",
    "    with tf.name_scope('val'):\n",
    "        val_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(val_out, axis=-1, output_type=tf.int32), val_labels), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "opt = tf.train.MomentumOptimizer(lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在定义训练的时候, 注意到我们使用了`batch_norm`层时,需要更新每一层的`average`和`variance`参数, 更新的过程不包含在正常的训练过程中, 需要我们去手动像下面这样更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过`tf.get_collection`获得所有需要更新的`op`\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "# 使用`tensorflow`的控制流, 先执行更新算子, 再执行训练\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = opt.minimize(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里, 我们把带`bn层`的训练过程封装在`utils.learning.train_with_bn`中, 感兴趣的同学可以看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.learning import train_with_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]: step 0 loss = 2.3080 acc = 0.0781 (0.0274 / batch)\n",
      "[val]: step 0 loss = 2.3094 acc = 0.0938\n",
      "[train]: step 1000 loss = 1.1208 acc = 0.5781 (0.0916 / batch)\n",
      "[train]: step 2000 loss = 0.7210 acc = 0.7500 (0.0895 / batch)\n",
      "[train]: step 3000 loss = 1.1458 acc = 0.6719 (0.0903 / batch)\n",
      "[train]: step 4000 loss = 0.5391 acc = 0.8281 (0.0896 / batch)\n",
      "[val]: step 4000 loss = 1.0456 acc = 0.6562\n",
      "[train]: step 5000 loss = 0.3767 acc = 0.8750 (0.0898 / batch)\n",
      "[train]: step 6000 loss = 0.3564 acc = 0.8750 (0.0899 / batch)\n",
      "[train]: step 7000 loss = 0.5875 acc = 0.7656 (0.0899 / batch)\n",
      "[train]: step 8000 loss = 0.5125 acc = 0.8281 (0.0895 / batch)\n",
      "[val]: step 8000 loss = 1.5897 acc = 0.6719\n",
      "[train]: step 9000 loss = 0.1365 acc = 0.9219 (0.0898 / batch)\n",
      "[train]: step 10000 loss = 0.0546 acc = 1.0000 (0.0900 / batch)\n",
      "[train]: step 11000 loss = 0.2409 acc = 0.9062 (0.0896 / batch)\n",
      "[train]: step 12000 loss = 0.1659 acc = 0.9375 (0.0896 / batch)\n",
      "[val]: step 12000 loss = 1.5567 acc = 0.6875\n",
      "[train]: step 13000 loss = 0.4453 acc = 0.8438 (0.0898 / batch)\n",
      "[train]: step 14000 loss = 0.3721 acc = 0.9375 (0.0901 / batch)\n",
      "[train]: step 15000 loss = 0.1110 acc = 0.9531 (0.0896 / batch)\n",
      "[train]: step 16000 loss = 0.0562 acc = 0.9688 (0.0897 / batch)\n",
      "[val]: step 16000 loss = 0.8156 acc = 0.8438\n",
      "[train]: step 17000 loss = 0.0660 acc = 0.9844 (0.0899 / batch)\n",
      "[train]: step 18000 loss = 0.1114 acc = 0.9688 (0.0898 / batch)\n",
      "[train]: step 19000 loss = 0.1885 acc = 0.9531 (0.0896 / batch)\n",
      "[train]: step 20000 loss = 0.1317 acc = 0.9688 (0.0898 / batch)\n",
      "[val]: step 20000 loss = 1.6431 acc = 0.7344\n",
      "-------------------------Over all Result-------------------------\n",
      "[TRAIN]: loss = 0.1935 acc = 0.9388\n",
      "[VAL]: loss = 1.5755 acc = 0.7399\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_with_bn(sess, train_op, train_loss, train_acc, val_loss, val_acc, 20000, is_training)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`InceptionNet`有很多的变体, 比如`InceptionV1`,`V2`, `V3`, `V4`版本, 尝试查看论文, 自己动手实现一下并比较他们的不同"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
