{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 做图像分类\n",
    "前面我们讲了 RNN 特别适合做序列类型的数据，那么 RNN 能不能想 CNN 一样用来做图像分类呢？下面我们用 mnist 手写字体的例子来展示一下如何用 RNN 做图像分类，但是这种方法并不是主流，这里我们只是作为举例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一张手写字体的图片，其大小是 28 * 28，我们可以将其看做是一个长为 28 的序列，每个序列的特征都是 28，也就是\n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tKfTcly1fmu7d0byfkj30n60djdg5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样我们解决了输入序列的问题，对于输出序列怎么办呢？其实非常简单，虽然我们的输出是一个序列，但是我们只需要保留其中一个作为输出结果就可以了，这样的话肯定保留最后一个结果是最好的，因为最后一个结果有前面所有序列的信息，就像下面这样\n",
    "\n",
    "![](https://ws3.sinaimg.cn/large/006tKfTcly1fmu7fpqri0j30c407yjr8.jpg)\n",
    "\n",
    "下面我们直接通过例子展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "\n",
    "from utils.layers import lstm\n",
    "\n",
    "tf.set_random_seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入`mnist`数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True, reshape=False)\n",
    "train_set = mnist.train\n",
    "test_set = mnist.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察一个批次数据的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs, train_labels = train_set.next_batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 28, 28, 1)\n",
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_imgs.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回忆之前我们在构造`rnn`的初始状态的时候需要指定`batch_size`, 在构造`RNNCell`的`dropout`的时候需要知道`keep_prob`. 它们在训练和测试的时候明显应当是不同的取值, 我们可以非常方便的用`占位符`来实现这种不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ph = tf.placeholder(shape=(None, 28, 28, 1), dtype=tf.float32)\n",
    "label_ph = tf.placeholder(shape=(None, 10), dtype=tf.int64)\n",
    "batch_size_ph = tf.placeholder(tf.int32, [])\n",
    "keep_prob_ph = tf.placeholder(tf.float32, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据转化成满足`RNN`输入的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.transpose(tf.squeeze(input_ph, axis=[-1]), (1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, ?, 28)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样第一维就是时间步长, 第二维是`batch_size`, 第三维是输入特征个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义`rnn`分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rnn_classify(inputs, rnn_units=100, rnn_layers=2, batch_size=64, keep_prob=1, num_classes=10):\n",
    "    # 构造一个多层`rnn`模型\n",
    "    rnn_out, rnn_state = lstm(inputs, rnn_units, rnn_layers, batch_size, keep_prob=keep_prob)\n",
    "    \n",
    "    # 取出最后一个输出当作分类层的输入特征向量\n",
    "    net = rnn_out[-1]\n",
    "    \n",
    "    # 最后连接一个分类层\n",
    "    net = slim.flatten(net)\n",
    "    net = slim.fully_connected(net, num_classes, activation_fn=None, scope='classification')\n",
    "    \n",
    "    return net\n",
    "\n",
    "out = rnn_classify(inputs, batch_size=batch_size_ph, keep_prob=keep_prob_ph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义`loss`和`train_op`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.softmax_cross_entropy(logits=out, onehot_labels=label_ph)\n",
    "\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(out, axis=-1), tf.argmax(label_ph, axis=-1)), dtype=tf.float32))\n",
    "\n",
    "lr = 0.01\n",
    "optimizer = tf.train.MomentumOptimizer(lr, 0.9)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1000: train_loss: 0.187749 train_acc: 0.953125 test_loss: 0.172851 test_acc: 0.953125\n",
      "STEP 2000: train_loss: 0.126416 train_acc: 0.968750 test_loss: 0.145504 test_acc: 0.953125\n",
      "STEP 3000: train_loss: 0.008995 train_acc: 1.000000 test_loss: 0.072065 test_acc: 0.968750\n",
      "STEP 4000: train_loss: 0.011813 train_acc: 1.000000 test_loss: 0.039175 test_acc: 0.992188\n",
      "STEP 5000: train_loss: 0.003073 train_acc: 1.000000 test_loss: 0.043307 test_acc: 0.984375\n",
      "STEP 6000: train_loss: 0.037937 train_acc: 0.984375 test_loss: 0.057352 test_acc: 0.976562\n",
      "STEP 7000: train_loss: 0.022462 train_acc: 0.984375 test_loss: 0.019089 test_acc: 0.992188\n",
      "STEP 8000: train_loss: 0.001094 train_acc: 1.000000 test_loss: 0.092557 test_acc: 0.968750\n",
      "STEP 9000: train_loss: 0.008558 train_acc: 1.000000 test_loss: 0.046139 test_acc: 0.976562\n",
      "STEP 10000: train_loss: 0.027353 train_acc: 1.000000 test_loss: 0.107489 test_acc: 0.976562\n",
      "Train Done!\n",
      "------------------------------\n",
      "Train loss: 0.033395\n",
      "Train accuracy: 0.989364\n",
      "Test loss: 0.055709\n",
      "Test accuracy: 0.985000\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(10000):\n",
    "    images, labels = train_set.next_batch(64)\n",
    "    sess.run(train_op, feed_dict={input_ph: images, label_ph: labels, batch_size_ph: 64, keep_prob_ph: 0.5})\n",
    "    if e % 1000 == 999:\n",
    "        test_imgs, test_labels = test_set.next_batch(128)\n",
    "        loss_train, acc_train = sess.run([loss, acc], feed_dict={input_ph: images, label_ph: labels, batch_size_ph: 64, keep_prob_ph: 1.0})\n",
    "        loss_test, acc_test = sess.run([loss, acc], feed_dict={input_ph: test_imgs, label_ph: test_labels, batch_size_ph: 128, keep_prob_ph: 1.0})\n",
    "        print('STEP {}: train_loss: {:.6f} train_acc: {:.6f} test_loss: {:.6f} test_acc: {:.6f}'.format(e + 1, loss_train, acc_train, loss_test, acc_test))\n",
    "\n",
    "print('Train Done!')\n",
    "print('-'*30)\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "for _ in range(train_set.num_examples // 100):\n",
    "    image, label = train_set.next_batch(100)\n",
    "    loss_train, acc_train = sess.run([loss, acc], feed_dict={input_ph: image, label_ph: label, batch_size_ph: 100, keep_prob_ph: 1.0})\n",
    "    train_loss.append(loss_train)\n",
    "    train_acc.append(acc_train)\n",
    "\n",
    "print('Train loss: {:.6f}'.format(np.array(train_loss).mean()))\n",
    "print('Train accuracy: {:.6f}'.format(np.array(train_acc).mean()))\n",
    "\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "for _ in range(test_set.num_examples // 100):\n",
    "    image, label = test_set.next_batch(100)\n",
    "    loss_test, acc_test = sess.run([loss, acc], feed_dict={input_ph: image, label_ph: label, batch_size_ph: 100, keep_prob_ph: 1.0})\n",
    "    test_loss.append(loss_test)\n",
    "    test_acc.append(acc_test)\n",
    "\n",
    "print('Test loss: {:.6f}'.format(np.array(test_loss).mean()))\n",
    "print('Test accuracy: {:.6f}'.format(np.array(test_acc).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，训练 10000 次在简单的 mnist 数据集上也取得的了 98% 的准确率，所以说 RNN 也可以做做简单的图像分类，但是这并不是他的主战场，下次课我们会讲到 RNN 的一个使用场景，时间序列预测。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
