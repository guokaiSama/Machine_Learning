{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Tensorflow`中的循环神经网络模块\n",
    "前面我们讲了循环神经网络的基础知识和网络结构，下面我们教大家如何在`Tensorflow`下构建循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一般的 RNN\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tKfTcly1fmt9xz889xj30kb07nglo.jpg)\n",
    "\n",
    "对于最简单的 RNN，我们可以使用下面两种方式去调用，分别是 `tf.nn.rnn_cell.BasicRNNCell()` 和 `tf.nn.dynamic_rnn`，这两种方式的区别在于 `BasicRNNCell()` 只能接受序列中单步的输入，且必须传入隐藏状态，而 `dynamic_rnn()` 可以接受一个序列的输入，默认会传入全 0 的隐藏状态，也可以自己申明隐藏状态传入。\n",
    "\n",
    "`BasicRNNCell()` 里面的参数有\n",
    "\n",
    "num_units 表示输出的特征维度\n",
    "\n",
    "activation 表示选用的激活函数, 默认`tanh`\n",
    "\n",
    "reuse 表示是否需要复用\n",
    "\n",
    "`dynamic_rnn` 里面的参数则要丰富一些, 最重要的参数是下面的这些:\n",
    "\n",
    "inputs: 基础的`RNNCell`\n",
    "\n",
    "initial_state: 设置初始状态\n",
    "\n",
    "time_major: 确定时间步长是否在输入的第一维上, 如果是, 那么输入就是`[max_time, batch_size, depth]`的格式, 否则是`[batch_size, max_step, depth]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义一个基本的`rnn`单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_single = tf.nn.rnn_cell.BasicRNNCell(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造一个序列, 长度为6, batch是5, 特征是100\n",
    "x = tf.random_normal([6, 5, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 获取零值初始状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = rnn_single.zero_state(5, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对时间循环输出`rnn`结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取初始状态\n",
    "state = init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for i in range(6):\n",
    "    # 在第一次调用`rnn_single`之后, 要重用它的元素\n",
    "    if i > 0: \n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    out, state = rnn_single(x[i], state)\n",
    "    outputs.append(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 200)\n",
      "[[-0.08367088  0.367444   -0.35727128  0.07697963  0.22096567 -0.7905002 ]\n",
      " [-0.14765547  0.3692004  -0.87884575 -0.18412888  0.19545026 -0.78687793]\n",
      " [ 0.06849612 -0.44757083  0.66436     0.7206783   0.9236665  -0.9050538 ]\n",
      " [ 0.9667159  -0.11896209  0.31628886  0.7022554   0.67670786  0.2391372 ]\n",
      " [ 0.8100337  -0.26930103  0.6792861   0.8418961   0.32777315  0.5718113 ]]\n"
     ]
    }
   ],
   "source": [
    "print(state.shape)\n",
    "print(sess.run(state)[:, :6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到, `BasicRNNCell`的状态是`[batch, num_units]`的, 我们需要在每个时间步手动去调用状态来进行结果的输出. 但这样代码显得繁琐, 我们可以通过`dynamic_rnn`将时间隐藏起来, 让它自动的去执行\n",
    "\n",
    "- `dynamic_rnn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, final_state = tf.nn.dynamic_rnn(rnn_single, x, initial_state=init_state, time_major=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 200)\n",
      "[[-0.13105357  0.6218285  -0.1542633   0.41749775  0.33325344  0.8441833 ]\n",
      " [ 0.6061442  -0.01632764 -0.63648415  0.14741157  0.83866674 -0.07472478]\n",
      " [ 0.11849802 -0.5349209  -0.7570268  -0.14428064 -0.07784802  0.676307  ]\n",
      " [ 0.8182295  -0.40963945  0.2435521  -0.55186915  0.77439296  0.44285777]\n",
      " [-0.23756386 -0.07700434 -0.43314373  0.59477204 -0.21045244 -0.03230155]]\n"
     ]
    }
   ],
   "source": [
    "print(final_state.shape)\n",
    "print(sess.run(final_state)[:, :6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 5, 200)\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现, `dynamic_rnn`会输出最后一步的状态和中间所有时间步的结果\n",
    "\n",
    "我们还可以自定义初始状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义初始状态由随机正态分布产生\n",
    "init_state = tf.random_normal([5, 200], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, final_state = tf.nn.dynamic_rnn(rnn_single, x, initial_state=init_state, time_major=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.893339    0.02327475  0.20775038 -0.431116    0.31973243 -0.85546297]\n",
      " [-0.33059323  0.06875347 -0.63824296 -0.7352885  -0.94200224  0.8279837 ]\n",
      " [ 0.00450966 -0.6838723   0.39354405 -0.39674369 -0.79787093  0.63675916]\n",
      " [ 0.5266747  -0.4207145  -0.16804664 -0.20833765 -0.6320187  -0.52643865]\n",
      " [ 0.45180762  0.16632737 -0.11658883  0.9452162  -0.02629685  0.3860733 ]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(final_state[:, :6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RNN`训练的过程中比较容易产生过拟合的现象, 为此我们需要添加`dropout`\n",
    "\n",
    "- `RNNCell`添加`dropout`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加`dropout`正则项\n",
    "def build_rnn(num_units, batch_size, keep_prob=1):\n",
    "    cell = tf.nn.rnn_cell.BasicRNNCell(num_units)\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "        \n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_cell = build_rnn(100, 3, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tKfTcly1fmt9qj3uhmj30iz07ct90.jpg)\n",
    "\n",
    "LSTM 和基本的 RNN 是一样的，他的参数也是相同，我们就不再赘述了\n",
    "\n",
    "在`RNN`中, 我们也可以定义深层网络, 就是通过`tf.nn.rnn_cell.MultiRNNCell`来实现, 调用它的方式非常简单, 构造一个`cell`的`list`作为参数传入就可以了\n",
    "\n",
    "- `LSTM`和`MultiRNNCell`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(num_units, num_layers, batch_size, keep_prob=1):\n",
    "    def build_cell(num_units):\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(num_units, reuse=tf.AUTO_REUSE)\n",
    "        cell= tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "        \n",
    "        return cell\n",
    "    \n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([build_cell(num_units) for _ in range(num_layers)])\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, init_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造一个2层的`lstm`模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell, lstm_init_state = build_lstm(100, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lstm, final_state =  tf.nn.dynamic_rnn(lstm_cell, x, initial_state=lstm_init_state, time_major=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们有了一个2层的模型, 每个模型的状态有`c`和`h`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0\n",
      "c.shape: (5, 100)\n",
      "h.shape: (5, 100)\n",
      "layer 1\n",
      "c.shape: (5, 100)\n",
      "h.shape: (5, 100)\n"
     ]
    }
   ],
   "source": [
    "for i, layer_state in enumerate(final_state):\n",
    "    print('layer {}'.format(i))\n",
    "    print('c.shape: {}'.format(layer_state.c.shape))\n",
    "    print('h.shape: {}'.format(layer_state.h.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出形式还是相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 5, 100)\n"
     ]
    }
   ],
   "source": [
    "print(lstm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 自定义状态初始化\n",
    "\n",
    "有时候我们不希望用零去初始化, 考虑到`lstm`状态的特殊性, `tensorflow`用`LSTMStatusTuple`表示一个`LSTMCell`的状态, 它的参数如下\n",
    "\n",
    "`c`: 状态`c`\n",
    "\n",
    "`h`: 状态`h`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用`tuple`和`for`来定义这个2层模型的初始化状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = tuple([tf.nn.rnn_cell.LSTMStateTuple(tf.random_normal([5, 100]), tf.random_normal([5, 100])) for _ in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0\n",
      "c.shape: (5, 100)\n",
      "h.shape: (5, 100)\n",
      "layer 1\n",
      "c.shape: (5, 100)\n",
      "h.shape: (5, 100)\n"
     ]
    }
   ],
   "source": [
    "for i, layer_state in enumerate(init_state):\n",
    "    print('layer {}'.format(i))\n",
    "    print('c.shape: {}'.format(layer_state.c.shape))\n",
    "    print('h.shape: {}'.format(layer_state.h.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm, final_state =  tf.nn.dynamic_rnn(lstm_cell, x, initial_state=init_state, time_major=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n",
    "![](https://ws3.sinaimg.cn/large/006tKfTcly1fmtaj38y9sj30io06bmxc.jpg)\n",
    "\n",
    "GRU 和前面讲的这两个是同样的道理，就不再细说，还是演示一下例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru(num_units, num_layers, batch_size, keep_prob=1):\n",
    "    def build_cell(num_units):\n",
    "        cell = tf.nn.rnn_cell.GRUCell(num_units, reuse=tf.AUTO_REUSE)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "            \n",
    "        return cell\n",
    "    \n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([build_cell(num_units) for _ in range(num_layers)])\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_cell, gru_init_state = build_gru(100, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'MultiRNNCellZeroState_1/DropoutWrapperZeroState/GRUCellZeroState/zeros:0' shape=(5, 100) dtype=float32>, <tf.Tensor 'MultiRNNCellZeroState_1/DropoutWrapperZeroState_1/GRUCellZeroState/zeros:0' shape=(5, 100) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(gru_init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru, final_state =  tf.nn.dynamic_rnn(gru_cell, x, initial_state=gru_init_state, time_major=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
