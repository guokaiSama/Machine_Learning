{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 做词性预测\n",
    "\n",
    "下面我们用例子来简单的说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from utils.layers import lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用下面简单的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [(\"The dog ate the apple\".split(),\n",
    "                  [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "                 (\"Everybody read that book\".split(), \n",
    "                  [\"NN\", \"V\", \"DET\", \"NN\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们需要对单词和标签进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "tags = []\n",
    "for context, tag in training_data:\n",
    "    for w in context:\n",
    "        words.append(w.lower())\n",
    "    for t in tag:\n",
    "        tags.append(t.lower())\n",
    "words = list(set(words))\n",
    "tags = list(set(tags))\n",
    "\n",
    "word_to_idx = dict(zip(words, range(len(words))))\n",
    "tag_to_idx = dict(zip(tags, range(len(tags))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 5,\n",
       " 'ate': 2,\n",
       " 'book': 7,\n",
       " 'dog': 6,\n",
       " 'everybody': 0,\n",
       " 'read': 4,\n",
       " 'that': 3,\n",
       " 'the': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'det': 2, 'nn': 0, 'v': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们对字母进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "chars = list(alphabet)\n",
    "\n",
    "char_to_idx = dict(zip(chars, range(len(chars))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'b': 1,\n",
       " 'c': 2,\n",
       " 'd': 3,\n",
       " 'e': 4,\n",
       " 'f': 5,\n",
       " 'g': 6,\n",
       " 'h': 7,\n",
       " 'i': 8,\n",
       " 'j': 9,\n",
       " 'k': 10,\n",
       " 'l': 11,\n",
       " 'm': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'p': 15,\n",
       " 'q': 16,\n",
       " 'r': 17,\n",
       " 's': 18,\n",
       " 't': 19,\n",
       " 'u': 20,\n",
       " 'v': 21,\n",
       " 'w': 22,\n",
       " 'x': 23,\n",
       " 'y': 24,\n",
       " 'z': 25}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.contrib.lookup.index_table_from_tensor`\n",
    "在`tensorflow`运行过程中, 我们无法使用`python`下的字典, 因为图的元素是一个张量而不是具体值, `python`无法返回.\n",
    "\n",
    "我们需要用到`tf.contrib.lookup.index_table_from_tensor`, 它能帮我们搭建从字符串到编码的映射关系\n",
    "\n",
    "- 首先, 根据映射关系构造一个`table`\n",
    "\n",
    "函数定义非常简单, 参数就是需要映射元素列表的`tensor`形式, 在这里我们设置为常量`tensor`. 这样列表中元素的每一项都被映射成自己的下标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_table = tf.contrib.lookup.index_table_from_tensor(tf.constant(words))\n",
    "tag_table = tf.contrib.lookup.index_table_from_tensor(tf.constant(tags))\n",
    "char_table = tf.contrib.lookup.index_table_from_tensor(tf.constant(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立`table`之后, 我们就可以输入一个对应映射关系的列表, 从而查找到它的下标.\n",
    "\n",
    "- 构建占位符, 等待填入元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ph = tf.placeholder(tf.string, [None,])\n",
    "tag_ph = tf.placeholder(tf.string, [None,])\n",
    "char_ph = tf.placeholder(tf.string, [None,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `lookup`\n",
    "\n",
    "- 调用`table`的`lookup`方法, 就可以找到对应的下标了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_code = word_table.lookup(word_ph)\n",
    "tag_code = tag_table.lookup(tag_ph)\n",
    "char_code = char_table.lookup(char_ph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看看实际效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(symbols):\n",
    "    return map(lambda x: x.lower(), symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**: 当定义了`table`形式的`tensor`后, 我们需要额外对这些`table`初始化一次, 非常简单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "填入一个单词, 查看每个字母对应的编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 15 15 11  4]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(char_code, feed_dict={char_ph: list(lower('apple'))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "填入一个句子, 查看每个单词对应的编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Everybody', 'read', 'that', 'book']\n",
      "[0 4 3 7]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[1][0])\n",
    "print(sess.run(word_code, feed_dict={word_ph: list(lower(training_data[1][0]))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建`seq-lstm`模型\n",
    "- 首先构建单个字符的 lstm 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_lstm(char_code, n_char, char_dim, char_hidden, scope='char_lstm', reuse=tf.AUTO_REUSE):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 嵌入\n",
    "        embeddings = tf.get_variable('embeddings', shape=(n_char, char_dim), \n",
    "                                          dtype=tf.float32, initializer=tf.random_uniform_initializer(minval=0.0, maxval=1.0))\n",
    "        char_embed = tf.nn.embedding_lookup(embeddings, char_code, name='embed')\n",
    "        \n",
    "        # 将输入满足`(seq, batch, feature)`条件， 这里`batch=1`\n",
    "        char_embed = tf.expand_dims(char_embed, axis=1)\n",
    "        \n",
    "        # 经过`lstm`网络给出特征\n",
    "        out, _ = lstm(char_embed, char_hidden, 1, 1)\n",
    "        \n",
    "        return out[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "辅助占位符, 用于后面的`while_loop`, 可能有更简洁的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_ph = tf.placeholder(tf.float32, shape=[None, None, None])\n",
    "aux = [[[-1]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构造词性分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_tagger(word_code, word_list, n_word, n_char, word_dim, char_dim, \n",
    "               word_hidden, char_hidden, n_tag, aux_ph=aux_ph, scope='lstm_tagger', reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # 首先对一个句子里的所有单词用`char_lstm`进行编码\n",
    "        def char_lstm_fun(single_word):\n",
    "            # 使用`tf.string_split`对单词进行字母级别的分割\n",
    "            char_list = tf.string_split([single_word], delimiter='').values\n",
    "            \n",
    "            # 使用`char_table`查找所有字母的编码\n",
    "            char_code = char_table.lookup(char_list)\n",
    "            \n",
    "            # 将编码进入`lstm`得到输出\n",
    "            char_lstm_out = char_lstm(char_code, len(chars), 10, char_hidden)\n",
    "\n",
    "            return char_lstm_out\n",
    "        \n",
    "        # `tf.while_loop`的循环体函数\n",
    "        def loop_body(i, char, word_list):\n",
    "            # 对第`i`个单词得到`lstm`的结果\n",
    "            char_lstm_out = char_lstm_fun(word_list[i])\n",
    "            \n",
    "            # 在`[seq, batch, feature]`的第一维上连接之前的结果\n",
    "            # 使用`tf.cond`处理第一次的情况\n",
    "            char = tf.cond(tf.equal(i, 0), lambda: tf.expand_dims(char_lstm_out, axis=0), lambda: tf.concat([char, tf.expand_dims(char_lstm_out, axis=0)], axis=0))\n",
    "            \n",
    "            # 循环参数自增1, 返回所有用到的变量\n",
    "            # 在这里, 由于`tensorflow`强制要求循环体的输入和输出必须具有相同的`dtype`, `shape`.\n",
    "            # 而我们一个句子中单词的数量是不定的, 所以这里需要一个`[None, None, None]`形状的`tensor`作为初始值传进来\n",
    "            # 当然可能有别的简洁方法, 比如`tf.scan`等\n",
    "            return i + 1, char, word_list\n",
    "        \n",
    "        # 得到循环终止条件\n",
    "        num_words = tf.shape(word_list)[0]\n",
    "        \n",
    "        # 使用`tf.while_loop`得到所有单词的`lstm`结果\n",
    "        # `tf.while_loop`调用风格类似C++\n",
    "        # 第一个参数是循环终止条件\n",
    "        # 第二个参数是循环体, 也就是每一步循环具体做什么\n",
    "        # 第三个参数是初始循环变量\n",
    "        _, char, _ = tf.while_loop(lambda i, char, separate_words: i < num_words, loop_body, [0, aux_ph, word_list], name='looop')\n",
    "        \n",
    "        # 循环结束后, 由于是和一个`[None, None, None]`进行连接, 因此没有形状\n",
    "        # 在这里, 固定住形状\n",
    "        char.set_shape((None, 1, char_hidden))\n",
    "        \n",
    "        # 构造单词的嵌入模型\n",
    "        word_embeddings = tf.get_variable('embeddings', shape=(n_word, word_dim), \n",
    "                                          dtype=tf.float32, initializer=tf.random_uniform_initializer(minval=0.0, maxval=1.0))\n",
    "        net = tf.nn.embedding_lookup(word_embeddings, word_code, name='word_embed')# (seq, word_dim)\n",
    "        net = tf.expand_dims(net, axis=1) # (seq, batch, word_dim)\n",
    "        \n",
    "        # 将单词的嵌入向量和单词的`lstm`结果按照最后一维(特征)进行连接\n",
    "        net = tf.concat([char, net], axis=-1)\n",
    "        \n",
    "        # 进入`lstm`\n",
    "        net, _ = lstm(net, word_hidden, 1, 1)\n",
    "        \n",
    "        # 分类层\n",
    "        net = tf.reshape(net, (-1, word_hidden))\n",
    "        net = slim.fully_connected(net, n_tag, activation_fn=None, scope='classification')\n",
    "        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = lstm_tagger(word_code, word_ph, len(words), len(chars), 100, 10, 128, 50, len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=tag_code, logits=net)\n",
    "\n",
    "opt = tf.train.MomentumOptimizer(1e-2, 0.9)\n",
    "train_op = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Loss: 0.404266\n",
      "Epoch: 100, Loss: 0.032966\n",
      "Epoch: 150, Loss: 0.012619\n",
      "Epoch: 200, Loss: 0.007405\n",
      "Epoch: 250, Loss: 0.005119\n",
      "Epoch: 300, Loss: 0.003861\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for e in range(300):\n",
    "    train_loss = 0\n",
    "    for word, tag in training_data:\n",
    "        curr_train_loss, _ = sess.run([loss, train_op], feed_dict={word_ph: list(lower(word)), tag_ph: list(lower(tag)), aux_ph: [[[-1]]]})\n",
    "        train_loss += curr_train_loss\n",
    "    if (e + 1) % 50 == 0:\n",
    "        print('Epoch: {}, Loss: {:.6f}'.format(e + 1, train_loss / len(training_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = 'Everybody ate the apple'\n",
    "out = sess.run(net, feed_dict={word_ph: list(lower(test_sent.split())), aux_ph: [[[-1]]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.608797  -1.0200145 -3.3418741]\n",
      " [-1.7799056  4.2054605 -2.1964931]\n",
      " [-3.2337868 -1.4500072  3.8858328]\n",
      " [ 4.277447  -2.6897924 -1.8216504]]\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nn': 0, 'v': 1, 'det': 2}\n"
     ]
    }
   ],
   "source": [
    "print(tag_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后可以得到上面的结果，因为最后一层的线性层没有使用 softmax，所以数值不太像一个概率，但是每一行数值最大的就表示属于该类，可以看到第一个单词 'Everybody' 属于 nn，第二个单词 'ate' 属于 v，第三个单词 'the' 属于det，第四个单词 'apple' 属于 nn，所以得到的这个预测结果是正确的"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
