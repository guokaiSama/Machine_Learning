{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 创作古诗\n",
    "在这一章中我们了解到循环神经网络非常擅长处理序列和自然语言处理，文本都是由单词或者汉字按照序列顺序组成的，那么如何能够生成文本呢？下面我们来讲一讲原理，需要你根据这个原理来实现整个网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原理介绍\n",
    "前面我们介绍过 RNN 的输入和输出存在多种关系，比如多个输入对一个输出，这个时候输入是一个序列，输出是一个分类结果，就像使用 RNN 做图像分类。\n",
    "\n",
    "这里我们使用 RNN 来生成文本，网络的输入是一个序列，同时输出也是一个相同长度的序列，结构如下\n",
    "\n",
    "<img src=https://ws1.sinaimg.cn/large/006tNc79gy1fob5kq3r8jj30mt09dq2r.jpg width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的网络流程中，输入是一个序列 \"床 前 明 月 光\"，输出也是一个序列 \"前 明 月 光 床\"。如果你仔细观察可以发现网络的每一步输出都是下一步的输入，这就是其设计思路。\n",
    "\n",
    "那么对于任意的一段话，比如 \"我喜欢小猫\"，我们可以将其拆分 \"我 喜 欢 小 猫\" 这个长度为 5 的序列，网络的每一步输出就是 \"喜 欢 小 猫 我\"，也就是每个字符的输出就是其**紧跟**的后一个字符。\n",
    "\n",
    "当然对于一个序列，其最后一个字符后面并没有其他的字符，所以有多种方式选择，比如将序列的第一个字符作为其输出，也就是 \"光\" 的输出是 \"床\"，或者将其本身作为输出，也就是 \"光\" 的输出是 \"光\"，这里的选择可以有很多，我们使用一种循环的连接，将第一个字符作为最后一个字符的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成文本\n",
    "这样设计网络的训练流程是为了非常好地生成文本，下面我们说明一下如何进行文本的生成。\n",
    "\n",
    "首先需要输入网络一段初始的序列进行预热，预热的过程并不需要实际的输出结果，只是为了生成拥有记忆效果的隐藏状态，并将隐藏状态保留下来，接着我们开始正式生成文本，每个字符作为输入都可以得到输出，然后将输出作为下一步的输入，这样就可以不断地生成新的句子，这个过程是可以无限循环下去，或者到达我们的要求输出长度，具体可以看看下面的图示\n",
    "\n",
    "<img src=https://ws2.sinaimg.cn/large/006tNc79gy1fob5z06w1uj30qh09m0sl.jpg width=800>\n",
    "\n",
    "讲完了原理之后，下面就该你亲自动手去实现这个网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们可以探索一下数据集是什么样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.315656Z",
     "start_time": "2018-02-18T03:28:52.286844Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./data/poetry.txt', 'r') as f:\n",
    "    poetry_corpus = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们取得了前100个字符的结果，其中 `\\n` 表示换行符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.331908Z",
     "start_time": "2018-02-18T03:28:52.317790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'寒随穷律变，春逐鸟声开。\\n初风飘带柳，晚雪间花梅。\\n碧林青旧竹，绿沼翠新苔。\\n芝田初雁去，绮树巧莺来。\\n晚霞聊自怡，初晴弥可喜。\\n日晃百花色，风动千林翠。\\n池鱼跃不同，园鸟声还异。\\n寄言博通者，知予物'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poetry_corpus[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.338277Z",
     "start_time": "2018-02-18T03:28:52.334069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总的字符数: 942681\n"
     ]
    }
   ],
   "source": [
    "# 看看字符数\n",
    "print('总的字符数: {}'.format(len(poetry_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "为了可视化比较方便，我们将换行字符 `\\n` 替换成空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.353185Z",
     "start_time": "2018-02-18T03:28:52.340405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒随穷律变 春逐鸟声开  初风飘带柳 晚雪间花梅  碧林青旧竹 绿沼翠新苔  芝田初雁去 绮树巧莺来  晚霞聊自怡 初晴弥可喜  日晃百花色 风动千林翠  池鱼跃不同 园鸟声还异  寄言博通者 知予物\n"
     ]
    }
   ],
   "source": [
    "poetry_corpus = poetry_corpus.replace('\\n', ' ').replace('\\r', ' ').replace('，', ' ').replace('。', ' ')\n",
    "print(poetry_corpus[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本数值表示\n",
    "对于每个文字，电脑并不能像人一样能够有效地识别，所以必须做一个转换，将文字转换成电脑能够识别的数字，相当于每个不同的汉字，都用不同的数字去表示，可以对所有非重复的字符，从 0 开始建立索引\n",
    "\n",
    "同时可能古诗中会出现一些生僻的字，这些字可能只会出现几次，甚至只会出现一次，引入这些字会增大模型的复杂度，同时也会影响模型的训练，可以将这些词频比较低的字去掉\n",
    "\n",
    "关于汉字和数字的转换，我们已经为你实现好了一个转换器，感兴趣的同学可以去 `utils.py` 中查看，在之后的练习中，你可以使用这个转换器进行生成文本的转换，下面我们先看看例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:52.642640Z",
     "start_time": "2018-02-18T03:28:52.355357Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.utils import TextConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.016322Z",
     "start_time": "2018-02-18T03:28:52.645616Z"
    }
   },
   "outputs": [],
   "source": [
    "convert = TextConverter('./data/poetry.txt', max_vocab=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们通过数据集建立好了这个转换器 `convert`，下面我们看看如何去调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.025196Z",
     "start_time": "2018-02-18T03:28:53.018514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始的文本结果: 寒随穷律变 春逐鸟声开\n",
      "\n",
      "转换成数字之后的结果: [ 40 166 358 936 565   0  10 367 108  63  78]\n",
      "\n",
      "将数字重新转换成文字: 寒随穷律变 春逐鸟声开\n"
     ]
    }
   ],
   "source": [
    "# 得到原始的文本结果\n",
    "txt_char = poetry_corpus[:11]\n",
    "print('原始的文本结果: {}'.format(txt_char))\n",
    "print()\n",
    "\n",
    "# 通过 convert 将文字转换成数字\n",
    "num_char = convert.text_to_arr(txt_char)\n",
    "print('转换成数字之后的结果: {}'.format(num_char))\n",
    "print()\n",
    "\n",
    "# 通过 convert 将数字转换成文字\n",
    "origin_txt_char = convert.arr_to_text(num_char)\n",
    "print('将数字重新转换成文字: {}'.format(origin_txt_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的例子，你可以看到，能够使用 `convert.text_to_arr` 对一个文本进行数字的转换，通过 `convert.arr_to_text` 将数字转换成文本 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造时序样本数据\n",
    "对于一整段文本，并不适合全部输入到循环神经网络中，因为我们前面了解到循环神经网络存在着长时依赖的问题，所以需要将整个文本分成很多个序列文本，然后将这些序列文本输入到循环神经网络中进行训练，只要我们定好每个序列的长度，那么序列个数也就被决定了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.036447Z",
     "start_time": "2018-02-18T03:28:53.027222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "序列的个数: 47134\n"
     ]
    }
   ],
   "source": [
    "# 每个序列的长度，你可以自行修改\n",
    "n_step = 20\n",
    "\n",
    "# 总的序列个数\n",
    "num_seq = int(len(poetry_corpus) / n_step)\n",
    "\n",
    "# 去掉最后不足一个序列长度的部分\n",
    "text = poetry_corpus[:num_seq*n_step]\n",
    "\n",
    "print('序列的个数: {}'.format(num_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着需要将序列中所有的文字转换成数字表示，同时重新排列成 **$(num\\_seq \\times n\\_step)$** 的矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成下面的 `#todo` 的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.921749Z",
     "start_time": "2018-02-18T03:28:53.260507Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6a99e8fe36e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m#todo: 使用 convert 将文本 text 转换成数字表示的数组\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m#todo: 将转换之后的数组重新排列成 (num_seq x n_step) 的形状\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "arr = None #todo: 使用 convert 将文本 text 转换成数字表示的数组\n",
    "arr = None #todo: 将转换之后的数组重新排列成 (num_seq x n_step) 的形状\n",
    "arr = arr.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-12197e5a9120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 不要修改下面的代码\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ================== test =================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Successful!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# 不要修改下面的代码\n",
    "# ================== test =================\n",
    "if arr.shape == (num_seq, n_step):\n",
    "    print('Successful!')\n",
    "else:\n",
    "    print('Failed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "据此，我们可以构建 Tensorflow 中的数据读取来训练网络，这里我们将最后一个字符的输出 label 定为输入的第一个字符，也就是\"床前明月光\"的输出是\"前明月光床\"，完成下面 #todo 的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.945768Z",
     "start_time": "2018-02-18T03:28:53.925488Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextDataset(object):\n",
    "    def __init__(self, arr):\n",
    "        self.arr = arr\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        #TODO: 取得 arr 中的 item 这一个序列\n",
    "        x = None\n",
    "\n",
    "        #TODO: 构造上述描述的 label\n",
    "        y = None\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.arr.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你构造好了这个数据集类，我们可以将其实例化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.950296Z",
     "start_time": "2018-02-18T03:28:53.947697Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set = TextDataset(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们可以取出其中一个数据集参看一下是否是我们描述的这样，这个数据集需要像上面描述的一样，请自行检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T03:28:53.957705Z",
     "start_time": "2018-02-18T03:28:53.952232Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y = train_set[0]\n",
    "print('输入的文字序列 x: {}'.format(convert.arr_to_text(x)))\n",
    "print('输出的文字序列 y: {}'.format(convert.arr_to_text(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立模型\n",
    "下面我们需要构建这个循环神经网路的网络结构，模型可以定义成非常简单的两层\n",
    "- 第一层是 RNN 层, **LSTM (GRU)**\n",
    "- 第二层是线性层，做分类问题，最后输出预测的字符 **slim.fully_connected**\n",
    "\n",
    "只需要按照提示填写下面的 #todo 部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构造输入\n",
    "\n",
    "    首先构造一些placeholder作为网络的输入,方便之后代入数据, 需要构建的是:\n",
    "    - inputs: placeholder, 接收 `[batch_size, n_step]` 的输入, 是输入的词\n",
    "    - targets: placeholder, 接收 `[batch_size, n_step]` 的输入, 是输入词的对应词, 也就是 label\n",
    "    - keep_prob: placeholder, 用来表示 dropout 的保留概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, n_step):\n",
    "    '''\n",
    "    \n",
    "    args:\n",
    "        batch_size: 一个批次中有多少个序列输入\n",
    "        num_steps: 一个序列中有多少个词\n",
    "        \n",
    "    return:\n",
    "        inputs: 输入的词\n",
    "        targets: 输入词的对应词\n",
    "        keep_prob: dropout 保留概率\n",
    "    '''\n",
    "    inputs = None\n",
    "    targets = None\n",
    "    keep_prob = None\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构造 RNN\n",
    "\n",
    "    然后我们开始构造 RNN, 将一个序列中的每个词产生一个输出词. \n",
    "\n",
    "    这里我们可以构造一个多层的 RNN, 可以使用 LSTM 或者 GRU 作为 RNN 的基本单元."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(hidden_size, num_layers, batch_size, keep_prob):\n",
    "    '''\n",
    "    \n",
    "    args:\n",
    "        keep_prob: dropout 保留概率\n",
    "        hidden_size: RNN 隐藏层大小\n",
    "        num_layers: RNN 隐藏层个数\n",
    "        batch_size: batch size\n",
    "\n",
    "    return:\n",
    "        cell: RNN cell\n",
    "        initial_state: RNN输入时的初始状态\n",
    "    '''\n",
    "    \n",
    "    def build_cell(hidden_size, keep_prob):\n",
    "        #todo: 得到一个 rnn cell, 可以是 rnn 或者 lstm 或者 gru\n",
    "        rnn = None\n",
    "        \n",
    "        #todo: 添加 dropout\n",
    "        cell = None\n",
    "        \n",
    "        return cell\n",
    "    \n",
    "    \n",
    "    #todo: 得到一个多层的 rnn cell\n",
    "    cell = None\n",
    "    \n",
    "    #todo: 得到 cell 的初始状态\n",
    "    initial_state = None\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构造分类层\n",
    "\n",
    "    现在我们要构造以 RNN 输出的结果为输入, 一个词为输出的全连接层, 这是一个分类问题, 有多少个词就是多少分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "def build_output(rnn_out, in_size, out_size):\n",
    "    '''\n",
    "    \n",
    "    args:\n",
    "        rnn_out: 上一步rnn的输出\n",
    "        in_size: rnn输出的特征个数\n",
    "        out_size: 词的总个数(分类数)\n",
    "    \n",
    "    return:\n",
    "        out: 输出词的概率向量\n",
    "        logits: softmax之前的结果\n",
    "    '''\n",
    "\n",
    "    #todo: rnn_out 的形状是 (batch, n_step, rnn_size), 将形状改成 (batch x n_step, rnn_size)\n",
    "    # 变成一个2阶矩阵才可以参与到下一步的分类层\n",
    "    x = None\n",
    "    \n",
    "    #todo: 一个全连阶层作为分类层\n",
    "    logits = None\n",
    "    \n",
    "    #todo: softmax 得到概率\n",
    "    out = None\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构造损失函数\n",
    "\n",
    "    这是一个分类问题, 因此我们使用 softmax_with_logits 作为损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, num_classes):\n",
    "    '''\n",
    "    \n",
    "    args:\n",
    "        logits: softmax之前的结果\n",
    "        targets: 目标词\n",
    "        num_classes: 词的总个数(分类数)\n",
    "    \n",
    "    return:\n",
    "        loss: loss tensor.\n",
    "    '''\n",
    "    \n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    #todo: softmax 分类损失函数\n",
    "    loss = None\n",
    "    \n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构造训练过程\n",
    "\n",
    "    接下来我们需要构造训练过程. 前面说到过, RNN 经常会遇到梯度爆炸的问题, 但有一种方法可以避免这类问题, 就是\"梯度裁剪\".\n",
    "    \n",
    "    tensorflow 可以通过**`tf.clip_by_global_norm(tensors, grad_clip)`**函数对`tensors`进行梯度裁剪. \n",
    "    \n",
    "    形象地说, 如果大于`grad_clip`就会将大于的部分剪掉, 这样操作之后, 所有的`tensors`都比`grad_clip`要小, 也就不会存在爆炸的问题了.\n",
    "    \n",
    "    因此在这里我们会用到这个方法, 那么我们就不再使用最简单的`optimizer.minimize`这个函数去构造训练过程了, 需要把这个过程拆开, 具体来说分为下面几步:\n",
    "    \n",
    "        - 计算所有可训练变量的梯度\n",
    "        - 对所有梯度进行裁剪\n",
    "        - 再将梯度应用到原来的变量上去\n",
    "        \n",
    "    第一步和第二步应该都知道如何去做, 第三步需要用到一个全新的函数, **`optimizer.apply_gradients`**. \n",
    "    \n",
    "    `optimizer`就是前面我们定义的比如说梯度下降法方法, Momentum方法, Adam方法等等优化器, 每个优化器都有`apply_gradients`方法, 这里不具体展开如何使用这个函数, 大家可以查看下面的函数说明或者参考[这里](https://tensorflow.google.cn/api_docs/python/tf/train/AdamOptimizer#apply_gradients)\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://image.ibb.co/dx7cRn/apply_gradient.png\" alt=\"apply gradient\" border=\"0\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    '''\n",
    "    \n",
    "    args:\n",
    "        loss: loss tensor\n",
    "        learning_rate: 学习率\n",
    "    \n",
    "    return:\n",
    "        optimizer: 优化方法\n",
    "    '''\n",
    "\n",
    "    #todo: 获取所有的可训练变量\n",
    "    tvars = None\n",
    "    \n",
    "    #todo: 获取 loss 对 tvars 的梯度\n",
    "    grads = None\n",
    "    \n",
    "    #todo: 使用 tf.clip_by_global_norm 进行梯度裁剪\n",
    "    grads_clipped = None\n",
    "    \n",
    "    #todo: 生成一个 Adam 优化器\n",
    "    train_op = None\n",
    "    \n",
    "    #todo: 使用 apply_gradients 生成一个参数更新 op\n",
    "    optimizer = None\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构建完整的 **CharRNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       rnn_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "        '''\n",
    "        \n",
    "        args:\n",
    "            num_classes: 分类数, 也就是字符总数\n",
    "            batch_size: batch size\n",
    "            num_steps: 一个序列中出现的字符个数\n",
    "            rnn_size: rnn 的隐藏层大小\n",
    "            num_layers: rnn 中的隐藏层个数\n",
    "            learning_rate: 学习率\n",
    "            grad_clip: 梯度裁剪常数\n",
    "            sampling: 是否进行采样\n",
    "            \n",
    "        '''\n",
    "        # 之后我们用这个网络进行 inference 的时候, 我们会传入一个字符进来, 而不是训练时候的\n",
    "        # 传入 n_step 个字符, 因此在这里用 sampling 来控制\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        #todo: 构建输入\n",
    "        self.inputs, self.targets, self.keep_prob = None\n",
    "\n",
    "        #todo: 构建RNN的cell\n",
    "        cell, self.initial_state = None\n",
    "\n",
    "        ### 用RNN跑一遍输入得到输出\n",
    "        # 首先将输入转化成one_hot形式, 相当于给字符编码\n",
    "        # 这里你也可以使用我们之前讲过的 word_embedding, 将字符嵌入到一个向量里面去\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        #todo: 运行RNN得到输出和最终状态(提示: 使用 tf.nn.dynamic_rnn)\n",
    "        outputs, state = None\n",
    "        \n",
    "        #todo: 将最后的状态保存在 final_state 中\n",
    "        self.final_state = state\n",
    "        \n",
    "        #todo: 得到分类层的结果\n",
    "        self.prediction, self.logits = None\n",
    "        \n",
    "        #todo: 得到损失函数\n",
    "        self.loss = None\n",
    "        \n",
    "        #todo: 得到优化算子\n",
    "        self.optimizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们再定义网络的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = None        # batch_size\n",
    "rnn_size = None          # rnn中隐藏层的大小\n",
    "num_layers = None        # rnn中隐藏层的个数\n",
    "learning_rate = None     # 学习率\n",
    "keep_prob = None         # dropout保留概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用上面的`CharRNN`构造model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "# Save every N iterations\n",
    "save_every_n = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面构造一个读取数据的`generator`, 也可以自行实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class build_data_generator:\n",
    "    def __init__(self, data, batch_size, shuffle=False):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.nb_of_examples = len(data)\n",
    "    \n",
    "    def __call__(self):\n",
    "        ind = 0\n",
    "        indices = list(range(self.nb_of_examples))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            random.shuffle(indices)\n",
    "\n",
    "        while ind + self.batch_size <= self.nb_of_examples:\n",
    "            x, y = self.data[ind: ind + batch_size]\n",
    "            ind += batch_size\n",
    "\n",
    "            yield x, y\n",
    "\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.nb_of_examples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=50)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        #todo: \n",
    "        #用 sess 去得到初始 state 保存在 new_state 中\n",
    "        new_state = None\n",
    "        \n",
    "        loss = 0\n",
    "        dataset = build_data_generator(train_set, batch_size, True)\n",
    "        \n",
    "        for x, y in dataset():\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            \n",
    "            #todo:\n",
    "            # 构造 feed_dict\n",
    "            # 这里, 我们需要得到model.inputs, model.targets, model.keep_prob, model.initial_state的输入\n",
    "            # 需要将上一步得到的state作为这一步的model.initial_state\n",
    "            feed = None\n",
    "            \n",
    "            #todo:\n",
    "            # 使用上面定义的 feed_dict, 运行 session, 获得当前 batch 的 loss, state, 并运行 model.optimizer\n",
    "            batch_loss, new_state, _ = None\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, rnn_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, rnn_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们可以查看在 checkpoints 中所有保存的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练完成之后, 可以从 checkpoints 中恢复模型, 然后我们再给网络输入一个字符, 再让 CharRNN 不断生成新的字符, 也就是让神经网络\"写诗\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让输出的字符更加丰富随意, 这里我们从模型输出的概率向量中随机选取前几个中的一个作为最后的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在是时候看看我们训练的神经网络能够写出什么样的诗了, 这里封装成了一个 sample 函数, 需要完成下面的 #todo 部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意, 我们在用 CharRNN 写诗的时候, 需要先用输入的字符对网络进行\"预热\", 这个过程我们不采用网络输出字符的结果, 但是可以得到一个更好的状态, 然后就可以用这个状态作为后续生成新字符的初始状态, 从而获得更好的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, rnn_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    \n",
    "    # 构造CharRNN模型\n",
    "    # 注意, 这里我们处于测试状态, batch_size 和 n_step 都应为1\n",
    "    model = CharRNN(convert.vocab_size, rnn_size=rnn_size, sampling=True)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "\n",
    "        #todo:\n",
    "        # 得到初始 state\n",
    "        new_state = None\n",
    "        \n",
    "        # 这一步我们先将输入的几个字符对网络进行\"预热\", \n",
    "        # 这样可以得到更好的 state\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = convert.word_to_int(c)\n",
    "            \n",
    "            #todo:\n",
    "            # 像之前一样设定feed_dict\n",
    "            feed = None\n",
    "            \n",
    "            #todo:\n",
    "            # 得到概率输出以及当前输出状态\n",
    "            preds, new_state = None\n",
    "\n",
    "        c = pick_top_n(preds, convert.vocab_size)\n",
    "        samples.append(convert.int_to_word(c))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            \n",
    "            #todo:\n",
    "            # 像之前一样设定feed_dict\n",
    "            feed = None\n",
    "            \n",
    "            #todo:\n",
    "            # 得到概率输出以及当前输出状态\n",
    "            preds, new_state = None\n",
    "\n",
    "            c = pick_top_n(preds, convert.vocab_size)\n",
    "            samples.append(convert.int_to_word(c))\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下训练时长最长的模型的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, convert.vocab_size, rnn_size, convert.vocab_size, prime=\"天青色等烟雨\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下刚刚开始训练时模型的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = 'checkpoints/i1000_l512.ckpt'\n",
    "samp = sample(checkpoint, convert.vocab_size, rnn_size, convert.vocab_size, prime=\"天青色等烟雨\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看模型的中间结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = 'checkpoints/i10000_l512.ckpt'\n",
    "samp = sample(checkpoint, convert.vocab_size, rnn_size, convert.vocab_size, prime=\"天青色等烟雨\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到, 随着模型不断训练, 得到的语句越来越丰富, 越来越完整, 也就是说效果越来越好. 但是本质上这还是一个概率模型, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
