{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基与经验回放池里不同的样本不同的采样权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 衰减因子GAMMA\n",
    "GAMMA = 0.9\n",
    "\n",
    "# EPSILON的初始值\n",
    "INITIAL_EPSILON = 0.5\n",
    "\n",
    "# EPSILON的最终值\n",
    "FINAL_EPSILON = 0.01\n",
    "#经验回放表的大小\n",
    "REPLAY_SIZE = 10000\n",
    "# 批量梯度下降的样本数m\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# 更新Q网络的频率\n",
    "REPLACE_TARGET_FREQ = 10\n",
    "\n",
    "# 迭代轮次T\n",
    "EPISODE = 3000\n",
    "STEP = 300\n",
    "TEST = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    data_pointer = 0\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # for all priority values\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        # [--------------Parent nodes-------------][-------leaves to recode priority-------]\n",
    "        #             size: capacity - 1                       size: capacity\n",
    "        self.data = np.zeros(capacity, dtype=object)  # for all transitions\n",
    "        # [--------------data frame-------------]\n",
    "        #             size: capacity\n",
    "\n",
    "    def add(self, p, data):\n",
    "        tree_idx = self.data_pointer + self.capacity - 1\n",
    "        self.data[self.data_pointer] = data  # update data_frame\n",
    "        self.update(tree_idx, p)  # update tree_frame\n",
    "\n",
    "        self.data_pointer += 1\n",
    "        if self.data_pointer >= self.capacity:  # replace when exceed the capacity\n",
    "            self.data_pointer = 0\n",
    "\n",
    "    def update(self, tree_idx, p):\n",
    "        change = p - self.tree[tree_idx]\n",
    "        self.tree[tree_idx] = p\n",
    "        # then propagate the change through tree\n",
    "        while tree_idx != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            tree_idx = (tree_idx - 1) // 2\n",
    "            self.tree[tree_idx] += change\n",
    "\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for transitions\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_idx = 0\n",
    "        while True:     # the while loop is faster than the method in the reference code\n",
    "            cl_idx = 2 * parent_idx + 1         # this leaf's left and right kids\n",
    "            cr_idx = cl_idx + 1\n",
    "            if cl_idx >= len(self.tree):        # reach bottom, end search\n",
    "                leaf_idx = parent_idx\n",
    "                break\n",
    "            else:       # downward search, always search for a higher priority node\n",
    "                if v <= self.tree[cl_idx]:\n",
    "                    parent_idx = cl_idx\n",
    "                else:\n",
    "                    v -= self.tree[cl_idx]\n",
    "                    parent_idx = cr_idx\n",
    "\n",
    "        data_idx = leaf_idx - self.capacity + 1\n",
    "        return leaf_idx, self.tree[leaf_idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total_p(self):\n",
    "        return self.tree[0]  # the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    epsilon = 0.01  # small amount to avoid zero priority\n",
    "    alpha = 0.6  # [0~1] convert the importance of TD error to priority\n",
    "    beta = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    beta_increment_per_sampling = 0.001\n",
    "    abs_err_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def store(self, transition):\n",
    "        max_p = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        if max_p == 0:\n",
    "            max_p = self.abs_err_upper\n",
    "        self.tree.add(max_p, transition)   # set the max p for new p\n",
    "\n",
    "    def sample(self, n):\n",
    "        b_idx, b_memory, ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, self.tree.data[0].size)), np.empty((n, 1))\n",
    "        pri_seg = self.tree.total_p / n       # priority segment\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])  # max = 1\n",
    "\n",
    "        min_prob = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_p     # for later calculate ISweight\n",
    "        if min_prob == 0:\n",
    "            min_prob = 0.00001\n",
    "        for i in range(n):\n",
    "            a, b = pri_seg * i, pri_seg * (i + 1)\n",
    "            v = np.random.uniform(a, b)\n",
    "            idx, p, data = self.tree.get_leaf(v)\n",
    "            prob = p / self.tree.total_p\n",
    "            ISWeights[i, 0] = np.power(prob/min_prob, -self.beta)\n",
    "            b_idx[i], b_memory[i, :] = idx, data\n",
    "        return b_idx, b_memory, ISWeights\n",
    "\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.epsilon  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.abs_err_upper)\n",
    "        ps = np.power(clipped_errors, self.alpha)\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    # DQN Agent\n",
    "    def __init__(self, env):\n",
    "        # init experience replay\n",
    "        self.replay_total = 0\n",
    "        # init some parameters\n",
    "        self.time_step = 0\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.memory = Memory(capacity=REPLAY_SIZE)\n",
    "\n",
    "        self.create_Q_network()\n",
    "        self.create_training_method()\n",
    "\n",
    "        # Init session\n",
    "        self.session = tf.InteractiveSession()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def create_Q_network(self):\n",
    "        # input layer\n",
    "        self.state_input = tf.placeholder(\"float\", [None, self.state_dim])\n",
    "        self.ISWeights = tf.placeholder(tf.float32, [None, 1])\n",
    "        # network weights\n",
    "        with tf.variable_scope('current_net'):\n",
    "            W1 = self.weight_variable([self.state_dim, 20])\n",
    "            b1 = self.bias_variable([20])\n",
    "            W2 = self.weight_variable([20, self.action_dim])\n",
    "            b2 = self.bias_variable([self.action_dim])\n",
    "\n",
    "            # hidden layers\n",
    "            h_layer = tf.nn.relu(tf.matmul(self.state_input, W1) + b1)\n",
    "            # Q Value layer\n",
    "            self.Q_value = tf.matmul(h_layer, W2) + b2\n",
    "\n",
    "        with tf.variable_scope('target_net'):\n",
    "            W1t = self.weight_variable([self.state_dim, 20])\n",
    "            b1t = self.bias_variable([20])\n",
    "            W2t = self.weight_variable([20, self.action_dim])\n",
    "            b2t = self.bias_variable([self.action_dim])\n",
    "\n",
    "            # hidden layers\n",
    "            h_layer_t = tf.nn.relu(tf.matmul(self.state_input, W1t) + b1t)\n",
    "            # Q Value layer\n",
    "            self.target_Q_value = tf.matmul(h_layer_t, W2t) + b2t\n",
    "\n",
    "        t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')\n",
    "        e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='current_net')\n",
    "\n",
    "        with tf.variable_scope('soft_replacement'):\n",
    "            self.target_replace_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]\n",
    "\n",
    "    def create_training_method(self):\n",
    "        self.action_input = tf.placeholder(\"float\", [None, self.action_dim])  # one hot presentation\n",
    "        self.y_input = tf.placeholder(\"float\", [None])\n",
    "        Q_action = tf.reduce_sum(tf.multiply(self.Q_value, self.action_input), reduction_indices=1)\n",
    "        self.cost = tf.reduce_mean(self.ISWeights * (tf.square(self.y_input - Q_action)))\n",
    "        self.abs_errors = tf.abs(self.y_input - Q_action)\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.0001).minimize(self.cost)\n",
    "\n",
    "    def store_transition(self, s, a, r, s_, done):\n",
    "        transition = np.hstack((s, a, r, s_, done))\n",
    "        self.memory.store(transition)  # have high priority for newly arrived transition\n",
    "\n",
    "    def perceive(self, state, action, reward, next_state, done):\n",
    "        one_hot_action = np.zeros(self.action_dim)\n",
    "        one_hot_action[action] = 1\n",
    "        # print(state,one_hot_action,reward,next_state,done)\n",
    "        self.store_transition(state, one_hot_action, reward, next_state, done)\n",
    "        self.replay_total += 1\n",
    "        if self.replay_total > BATCH_SIZE:\n",
    "            self.train_Q_network()\n",
    "\n",
    "    def train_Q_network(self):\n",
    "        self.time_step += 1\n",
    "        # Step 1: obtain random minibatch from replay memory\n",
    "        tree_idx, minibatch, ISWeights = self.memory.sample(BATCH_SIZE)\n",
    "        state_batch = minibatch[:, 0:4]\n",
    "        action_batch = minibatch[:, 4:6]\n",
    "        reward_batch = [data[6] for data in minibatch]\n",
    "        next_state_batch = minibatch[:, 7:11]\n",
    "        # Step 2: calculate y\n",
    "        y_batch = []\n",
    "        current_Q_batch = self.Q_value.eval(feed_dict={self.state_input: next_state_batch})\n",
    "        max_action_next = np.argmax(current_Q_batch, axis=1)\n",
    "        target_Q_batch = self.target_Q_value.eval(feed_dict={self.state_input: next_state_batch})\n",
    "\n",
    "        for i in range(0, BATCH_SIZE):\n",
    "            done = minibatch[i][11]\n",
    "            if done:\n",
    "                y_batch.append(reward_batch[i])\n",
    "            else:\n",
    "                target_Q_value = target_Q_batch[i, max_action_next[i]]\n",
    "                y_batch.append(reward_batch[i] + GAMMA * target_Q_value)\n",
    "\n",
    "        self.optimizer.run(feed_dict={\n",
    "            self.y_input: y_batch,\n",
    "            self.action_input: action_batch,\n",
    "            self.state_input: state_batch,\n",
    "            self.ISWeights: ISWeights\n",
    "        })\n",
    "        _, abs_errors, _ = self.session.run([self.optimizer, self.abs_errors, self.cost], feed_dict={\n",
    "            self.y_input: y_batch,\n",
    "            self.action_input: action_batch,\n",
    "            self.state_input: state_batch,\n",
    "            self.ISWeights: ISWeights\n",
    "        })\n",
    "        self.memory.batch_update(tree_idx, abs_errors)  # update priority\n",
    "\n",
    "    def egreedy_action(self, state):\n",
    "        Q_value = self.Q_value.eval(feed_dict={\n",
    "            self.state_input: [state]\n",
    "        })[0]\n",
    "        if random.random() <= self.epsilon:\n",
    "            self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / 10000\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / 10000\n",
    "            return np.argmax(Q_value)\n",
    "\n",
    "    def action(self, state):\n",
    "        return np.argmax(self.Q_value.eval(feed_dict={\n",
    "            self.state_input: [state]\n",
    "        })[0])\n",
    "\n",
    "    def update_target_q_network(self, episode):\n",
    "        # update target Q netowrk\n",
    "        if episode % REPLACE_TARGET_FREQ == 0:\n",
    "            self.session.run(self.target_replace_op)\n",
    "            # print('episode '+str(episode) +', target Q network params replaced!')\n",
    "\n",
    "    def weight_variable(self, shape):\n",
    "        initial = tf.truncated_normal(shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.constant(0.01, shape=shape)\n",
    "        return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 Evaluation Average Reward: 10.0\n",
      "episode:  100 Evaluation Average Reward: 31.4\n",
      "episode:  200 Evaluation Average Reward: 200.0\n",
      "episode:  300 Evaluation Average Reward: 200.0\n",
      "episode:  400 Evaluation Average Reward: 198.8\n",
      "episode:  500 Evaluation Average Reward: 195.6\n",
      "episode:  600 Evaluation Average Reward: 200.0\n",
      "episode:  700 Evaluation Average Reward: 200.0\n",
      "episode:  800 Evaluation Average Reward: 191.6\n",
      "episode:  900 Evaluation Average Reward: 180.0\n",
      "episode:  1000 Evaluation Average Reward: 193.4\n",
      "episode:  1100 Evaluation Average Reward: 165.0\n",
      "episode:  1200 Evaluation Average Reward: 179.0\n",
      "episode:  1300 Evaluation Average Reward: 187.2\n",
      "episode:  1400 Evaluation Average Reward: 191.4\n",
      "episode:  1500 Evaluation Average Reward: 92.2\n",
      "episode:  1600 Evaluation Average Reward: 168.8\n",
      "episode:  1700 Evaluation Average Reward: 163.4\n",
      "episode:  1800 Evaluation Average Reward: 128.4\n",
      "episode:  1900 Evaluation Average Reward: 132.4\n"
     ]
    }
   ],
   "source": [
    "# initialize OpenAI Gym env and dqn agent\n",
    "env = gym.make('CartPole-v0')\n",
    "agent = DQN(env)\n",
    "\n",
    "for episode in range(EPISODE):\n",
    "    # initialize task\n",
    "    state = env.reset()\n",
    "    # Train\n",
    "    for step in range(STEP):\n",
    "        action = agent.egreedy_action(state)  # e-greedy action for train\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # Define reward for agent\n",
    "        reward = -1 if done else 0.1\n",
    "        agent.perceive(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    # Test every 100 episodes\n",
    "    if episode % 100 == 0:\n",
    "        total_reward = 0\n",
    "        for i in range(TEST):\n",
    "            state = env.reset()\n",
    "            for j in range(STEP):\n",
    "                render = lambda : plt.imshow(env.render(mode='rgb_array'))\n",
    "                action = agent.action(state)  # direct action for test\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "        ave_reward = total_reward / TEST\n",
    "        print ('episode: ', episode, 'Evaluation Average Reward:', ave_reward)\n",
    "    agent.update_target_q_network(episode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
